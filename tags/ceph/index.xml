<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ceph on Buffer Overflow</title><link>/tags/ceph/</link><description>Recent content in Ceph on Buffer Overflow</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 16 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/ceph/index.xml" rel="self" type="application/rss+xml"/><item><title>The (Hyper)Convergence - Ceph + Proxmox</title><link>/posts/hyperconvergence-ceph-proxmox-homelab/</link><pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate><guid>/posts/hyperconvergence-ceph-proxmox-homelab/</guid><description>&lt;p>When I kicked off the latest iteration of my homelab project about 10 years ago, everything was harder. Shared storage was a luxury that meant diving into expensive SAN solutions which were neither feasible nor affordable for anyone not running a data center. Containers were still in buzzword territory, and their real-world application was confined either to early versions of Docker (pre-OCI, mind you!), or to cutting-edge cloud-native projects like Google&amp;rsquo;s Borg.&lt;/p></description><content>&lt;p>When I kicked off the latest iteration of my homelab project about 10 years ago, everything was harder. Shared storage was a luxury that meant diving into expensive SAN solutions which were neither feasible nor affordable for anyone not running a data center. Containers were still in buzzword territory, and their real-world application was confined either to early versions of Docker (pre-OCI, mind you!), or to cutting-edge cloud-native projects like Google&amp;rsquo;s Borg.&lt;/p>
&lt;p>At that time, virtualization was the future and VMware was the name in the game. In my workplace, vSphere was so deeply entrenched in our infrastructure that it felt almost absurd to think about running anything else for my homelab. VMware’s reliability, integrations, and comprehensive feature set made it an obvious choice for enterprises. For a home lab, though, it wasn’t just about running VMs - it was about learning the ropes of managing real enterprise-grade infrastructure. Thanks to generous licensing through the VMware Users Group, I ran VMware in my home lab for several years before the growing overhead of the vSphere stack pushed me to look for something else. Perhaps the most painful part of trying to run VMware at home was the lack of reliable shared storage.&lt;/p>
&lt;p>Here&amp;rsquo;s a list of all the solutions I tried:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NFS&lt;/strong> - NFS is the universal standard solution for &amp;ldquo;I have a file here and I want to use it over there&amp;rdquo;. Unfortunately, its performance is simply &lt;em>not great&lt;/em> when it comes to high IO and synchronous workloads.&lt;/li>
&lt;li>&lt;strong>Mounting iSCSI storage to ESXi using targetcli&lt;/strong> - This should have worked great, as iSCSI is clearly an industry standard. But the issue here was resiliency. I needed a physical machine to host the disks and present LUNs to the ESXi nodes, but that meant one more machine that couldn&amp;rsquo;t be virtualized and was effectively another critical Jenga block in my infrastructure.&lt;/li>
&lt;li>&lt;strong>One Hypervisor To Rule Them All&lt;/strong> - Putting all my eggs in one basket; a single hypervisor storing all my important VMs, with the other dedicated to only running the vSphere appliance. I don&amp;rsquo;t think I need to explain why this was a bad idea!&lt;/li>
&lt;/ul>
&lt;p>Fast forward to today, and my homelab has changed in ways I never imagined. Technologies that were once out of reach, like hyperconverged infrastructure and Ceph storage, are a major component of my current home lab. Proxmox’s built-in support for Ceph has made it so much easier to get a resilient storage system up and running, all without the insane hardware costs or complexity that used to be a given. It’s a reminder of how far the tech has come—and how much easier it’s become to experiment with advanced setups like this. What used to be complex, expensive, and requiring a Ph.D. in Storage-ology now feels almost effortless. That’s progress!&lt;/p>
&lt;h3 id="downgrading-hardware-upgrading-expectations">Downgrading Hardware, Upgrading Expectations&lt;/h3>
&lt;p>When I decided to finally migrate off of ESXi, I took that opportunity to redesign my VM storage solution to a distributed model. Instead of a vSphere cluster reading from a central NFS server, I set up a cluster of small, inexpensive micro workstations with fast local SSDs in a Ceph storage pool. This &lt;a href="https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster">hyperconverged model&lt;/a> handles both block storage (VM disks) and file storage (ISOs) with configurable replication to survive the loss of any single host.&lt;/p>
&lt;p>More importantly, the micro workstations each consume only ~20W under normal operating conditions. Compared to a traditional rackmount server which runs around 200W at idle, I&amp;rsquo;m saving around 140W per hour while eliminating a major single point of failure in my infrastructure.&lt;/p>
&lt;p>It’s been a fun project seeing what this mini powerhouse can handle. I’m using it for everything from highly-available VMs to LXD containerized applications, and it’s holding up incredibly well. Ceph also has native CSI integrations for Kubernetes, which means any workloads I run in my &lt;a href="/posts/whats-in-my-lab/">Kubernetes cluster&lt;/a> can benefit from fast, fault-tolerant network storage.&lt;/p></content></item></channel></rss>