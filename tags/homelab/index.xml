<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Homelab on Buffer Overflow</title><link>/tags/homelab/</link><description>Recent content in Homelab on Buffer Overflow</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 03 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/homelab/index.xml" rel="self" type="application/rss+xml"/><item><title>What's In My Lab Now, 2024 Edition</title><link>/posts/whats-in-my-lab-now-2024/</link><pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate><guid>/posts/whats-in-my-lab-now-2024/</guid><description>&lt;p>Almost four years ago (agh!) I published &lt;a href="/posts/whats-in-my-lab/">What&amp;rsquo;s In My Lab&lt;/a>, an overview of the systems and software I use at home for my own personal infrastructure. You might think that at a certain point you achieve some kind of serenity, and the desire to modify or expand is sated. Hah! Guess again.&lt;/p></description><content>&lt;p>Almost four years ago (agh!) I published &lt;a href="/posts/whats-in-my-lab/">What&amp;rsquo;s In My Lab&lt;/a>, an overview of the systems and software I use at home for my own personal infrastructure. You might think that at a certain point you achieve some kind of serenity, and the desire to modify or expand is sated. Hah! Guess again.&lt;/p>
&lt;h3 id="hardware">Hardware&lt;/h3>
&lt;p>I&amp;rsquo;ve continued with the trend of using low-power off-the-shelf systems. The beauty of a product line like Dell&amp;rsquo;s OptiPlex Micro is that the design is essentially unchanging from one generation to the next. Where I used to have a range of models - 7040, 5050, etc. - I&amp;rsquo;ve slowly upgraded them all to the same generation of 3060s. One extra feature I&amp;rsquo;ve added that you won&amp;rsquo;t find on a stock OptiPlex Micro is a &lt;a href="https://www.dfrobot.com/product-2318.html">secondary NIC from DFRobot&lt;/a> connected to the WiFi M.2 slot, giving each Micro an extra 1GbE port for redundancy or a dedicated storage network.&lt;/p>
&lt;p>There are only so many hard drives you can cram into an OptiPlex Micro - one, to be exact, of the 2.5&amp;quot; variety - so my storage needs are still served by a 2U SuperMicro rackmount server sporting 12 3.5&amp;quot; SAS drives split across two ZFS pools.&lt;/p>
&lt;p>Since my last post, I&amp;rsquo;ve also added a Home Automation server built on a &lt;a href="https://www.seeedstudio.com/reThings-reServer-c-2006.html">SeeedStudio reServer&lt;/a>, a vertical small form factor machine with space for two 3.5&amp;quot; SATA hard drives and an array of connectivity and expansion options tailored for edge computing applications. This server runs HomeAssistant, Z-Wave and Zigbee hubs, and the Frigate NVR software which uses Intel&amp;rsquo;s Rocket Lake integrated GPU to perform object detection and inferencing using the OpenVINO framework.&lt;/p>
&lt;p>I also have two vintage systems from SGI and Sun which I occasionally power on and take for a spin.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;strong>System&lt;/strong>&lt;/th>
&lt;th style="text-align: left">&lt;strong>CPU&lt;/strong>&lt;/th>
&lt;th style="text-align: left">&lt;strong>Memory&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">OptiPlex 3060&lt;/td>
&lt;td style="text-align: left">Intel Core i5-8500T&lt;/td>
&lt;td style="text-align: left">32GB DDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OptiPlex 3060&lt;/td>
&lt;td style="text-align: left">Intel Core i5-8500T&lt;/td>
&lt;td style="text-align: left">32GB DDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OptiPlex 5060&lt;/td>
&lt;td style="text-align: left">Intel Core i5-8500T&lt;/td>
&lt;td style="text-align: left">32GB DDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">SuperMicro CSE-826&lt;/td>
&lt;td style="text-align: left">Dual Intel Xeon E5-2630v4&lt;/td>
&lt;td style="text-align: left">64GB DDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">reServer i31115&lt;/td>
&lt;td style="text-align: left">Intel Core i3 1115G4&lt;/td>
&lt;td style="text-align: left">32GB DDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Raspberry Pi 5&lt;/td>
&lt;td style="text-align: left">ARM Cortex-A76 2.4GHz&lt;/td>
&lt;td style="text-align: left">8GB LPDDR4X&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Sun Netra X1&lt;/td>
&lt;td style="text-align: left">400MHz UltraSPARC IIe&lt;/td>
&lt;td style="text-align: left">1GB PC133&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Cobalt &amp;ldquo;RaQFive&amp;rdquo;&lt;/td>
&lt;td style="text-align: left">StarFive JH7110 SoC 1.5GHz&lt;/td>
&lt;td style="text-align: left">8GB DDR LPDDR4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">SGI O2&lt;/td>
&lt;td style="text-align: left">MIPS R5000 180MHz&lt;/td>
&lt;td style="text-align: left">1GB 133MHz SDRAM&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="hypervisors">Hypervisors&lt;/h3>
&lt;p>&lt;a href="https://www.proxmox.com/">Proxmox VE&lt;/a> continues to punch well above its weight class.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2024/04/homelab_2024.png" />
&lt;/figure>
&lt;h3 id="services">Services&lt;/h3>
&lt;p>Not much has changed here, but some services have been retired and replaced with other products.&lt;/p>
&lt;ul>
&lt;li>HAProxy
&lt;ul>
&lt;li>Three very lightweight LXD containers serving as a virtual network load balancer&lt;/li>
&lt;li>HAProxy acts as a TCP and HTTP load balancer&lt;/li>
&lt;li>Keepalived manages virtual IPs that failover between the two containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MariaDB Galera Cluster
&lt;ul>
&lt;li>Three Debian LXD containers running MariaDB with Galera multi-master replication&lt;/li>
&lt;li>Clients access the MariaDB cluster using a VIP and server pool managed by the HAProxy cluster&lt;/li>
&lt;li>I wrote a &lt;a href="https://github.com/danclough/mysql-healthcheck">monitoring daemon in Go&lt;/a> that provides HTTP healthcheck capabilities for HAProxy to determine the health of each MariaDB instance&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ceph
&lt;ul>
&lt;li>Each Proxmox host runs a Ceph mon (monitor), mgr (manager), and mds (metadata service)&lt;/li>
&lt;li>Each host also runs a Ceph OSD on an internal 2.5&amp;quot; enterprise SATA SSD&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Active Directory
&lt;ul>
&lt;li>Two Windows Server 2022 VMs across each host provide Active Directory Domain Services and DNS&lt;/li>
&lt;li>One Windows Server 2022 VM hosts an Active Directory Certificate Services intermediate CA for my &lt;a href="/posts/implementing-a-private-ca-for-home-use/">private CA hierarchy&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kubernetes
&lt;ul>
&lt;li>3 worker nodes running &lt;a href="https://k3s.io/">K3s&lt;/a>, a lightweight edge-focused Kubernetes distribution&lt;/li>
&lt;li>The K3s cluster is highly-available, with the K8s API accessible through the HAProxy load balancer cluster&lt;/li>
&lt;li>Workloads on Kubernetes can leverage the external MariaDB and Ceph clusters for persistence&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In addition to these clustered services, I also deployed a few services for better management and visibility across my environment:&lt;/p>
&lt;ul>
&lt;li>Ansible AWX
&lt;ul>
&lt;li>Manages automation for VMs, LXD containers, and physical hosts&lt;/li>
&lt;li>I use a number of open-source roles from the Ansible Galaxy community, and even contribute bugfixes and improvements to a few as I&amp;rsquo;m able!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Graylog Open
&lt;ul>
&lt;li>I use a few different Graylog inputs to receive data from all of my endpoints:
&lt;ul>
&lt;li>Syslog data from rsyslog&lt;/li>
&lt;li>GELF from Docker engines and Kubernetes pods&lt;/li>
&lt;li>Beats for Windows event logging with Winlogbeat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Graylog&amp;rsquo;s Sidecar functionality made it significantly easier to manage my Winlogbeat collectors in a central place, rather than relying on deploying winlogbeat via GPO!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>InfluxDB
&lt;ul>
&lt;li>Aggregates time-series monitoring data from telegraf clients on Linux&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Grafana
&lt;ul>
&lt;li>Observability into my entire infrastructure&lt;/li>
&lt;li>Presents metrics from InfluxDB in customized dashboards for each of my home lab services&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>Raspberry Pi 5's NVMe is a Killer Feature</title><link>/posts/pi-5-nvme-killer-feature/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>/posts/pi-5-nvme-killer-feature/</guid><description>&lt;p>Running a Kubernetes cluster on a few Raspberry Pi 4s has been a rewarding but challenging experience. The biggest limitation was storage - each Pi was booting from an SD card, which, while convenient, proved to be a massive bottleneck. SD cards are not only slow, but also wear out quickly under constant read/write operations. Over time, I experienced several cases of data corruption which brought that cluster to its knees.&lt;/p></description><content>&lt;p>Running a Kubernetes cluster on a few Raspberry Pi 4s has been a rewarding but challenging experience. The biggest limitation was storage - each Pi was booting from an SD card, which, while convenient, proved to be a massive bottleneck. SD cards are not only slow, but also wear out quickly under constant read/write operations. Over time, I experienced several cases of data corruption which brought that cluster to its knees.&lt;/p>
&lt;p>In fact, I&amp;rsquo;ve got almost 1TB worth of dead (or dying) MicroSD cards in a bench drawer from my endless sacrifices to the Non-Volatile Memory Gods. Constant awareness of this flaw from the resulting expensive heap of silicon pictured below made it very hard to me to trust that cluster for anything beyond experimentation.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2024/02/sdcards.png" />
&lt;/figure>
&lt;p>When the Raspberry Pi 5 was announced, its PCIe 3.0 lane immediately caught my eye. I&amp;rsquo;ve toyed with USB-attached SATA and NVMe options in the past, but the limitations and fickleness of USB always made those solutions prone to failure in any number of fun and unpredictable ways. After years of begging and pleading with SD cards and eMMC modules to behave themselves, we finally have a solution! When I took delivery of my Pi5, I paired it with a new &lt;a href="https://argon40.com/products/argon-one-v3-m-2-nvme-case">Argon ONE V3 NVMe case&lt;/a> and was incredibly pleased with the result.&lt;/p>
&lt;p>By supporting NVMe storage as a boot option, the Pi 5 significantly improves both performance and durability, making it much more feasible to run small-scale workloads without fear of data loss or SD card failure. This improvement opens up exciting possibilities for homelabs - for small labs and test clusters, it&amp;rsquo;s no longer an ill-fated endeavour to move storage-intensive applications like OpenSearch and Graylog from a traditional x86 rack server to more compact and power-efficient boards like the Pi. My hyperconverged Ceph cluster on Proxmox, which I used to present durable storage to the KubePi cluster, can go back to being mostly idle save for a small number of highly-available VMs running on the hypervisors. For me, this upgrade means my homelab cluster can move beyond hobby-level projects and start handling more serious workloads.&lt;/p></content></item><item><title>The (Hyper)Convergence - Ceph + Proxmox</title><link>/posts/hyperconvergence-ceph-proxmox-homelab/</link><pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate><guid>/posts/hyperconvergence-ceph-proxmox-homelab/</guid><description>&lt;p>When I kicked off the latest iteration of my homelab project about 10 years ago, everything was harder. Shared storage was a luxury that meant diving into expensive SAN solutions which were neither feasible nor affordable for anyone not running a data center. Containers were still in buzzword territory, and their real-world application was confined either to early versions of Docker (pre-OCI, mind you!), or to cutting-edge cloud-native projects like Google&amp;rsquo;s Borg.&lt;/p></description><content>&lt;p>When I kicked off the latest iteration of my homelab project about 10 years ago, everything was harder. Shared storage was a luxury that meant diving into expensive SAN solutions which were neither feasible nor affordable for anyone not running a data center. Containers were still in buzzword territory, and their real-world application was confined either to early versions of Docker (pre-OCI, mind you!), or to cutting-edge cloud-native projects like Google&amp;rsquo;s Borg.&lt;/p>
&lt;p>At that time, virtualization was the future and VMware was the name in the game. In my workplace, vSphere was so deeply entrenched in our infrastructure that it felt almost absurd to think about running anything else for my homelab. VMware’s reliability, integrations, and comprehensive feature set made it an obvious choice for enterprises. For a home lab, though, it wasn’t just about running VMs - it was about learning the ropes of managing real enterprise-grade infrastructure. Thanks to generous licensing through the VMware Users Group, I ran VMware in my home lab for several years before the growing overhead of the vSphere stack pushed me to look for something else. Perhaps the most painful part of trying to run VMware at home was the lack of reliable shared storage.&lt;/p>
&lt;p>Here&amp;rsquo;s a list of all the solutions I tried:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NFS&lt;/strong> - NFS is the universal standard solution for &amp;ldquo;I have a file here and I want to use it over there&amp;rdquo;. Unfortunately, its performance is simply &lt;em>not great&lt;/em> when it comes to high IO and synchronous workloads.&lt;/li>
&lt;li>&lt;strong>Mounting iSCSI storage to ESXi using targetcli&lt;/strong> - This should have worked great, as iSCSI is clearly an industry standard. But the issue here was resiliency. I needed a physical machine to host the disks and present LUNs to the ESXi nodes, but that meant one more machine that couldn&amp;rsquo;t be virtualized and was effectively another critical Jenga block in my infrastructure.&lt;/li>
&lt;li>&lt;strong>One Hypervisor To Rule Them All&lt;/strong> - Putting all my eggs in one basket; a single hypervisor storing all my important VMs, with the other dedicated to only running the vSphere appliance. I don&amp;rsquo;t think I need to explain why this was a bad idea!&lt;/li>
&lt;/ul>
&lt;p>Fast forward to today, and my homelab has changed in ways I never imagined. Technologies that were once out of reach, like hyperconverged infrastructure and Ceph storage, are a major component of my current home lab. Proxmox’s built-in support for Ceph has made it so much easier to get a resilient storage system up and running, all without the insane hardware costs or complexity that used to be a given. It’s a reminder of how far the tech has come—and how much easier it’s become to experiment with advanced setups like this. What used to be complex, expensive, and requiring a Ph.D. in Storage-ology now feels almost effortless. That’s progress!&lt;/p>
&lt;h3 id="downgrading-hardware-upgrading-expectations">Downgrading Hardware, Upgrading Expectations&lt;/h3>
&lt;p>When I decided to finally migrate off of ESXi, I took that opportunity to redesign my VM storage solution to a distributed model. Instead of a vSphere cluster reading from a central NFS server, I set up a cluster of small, inexpensive micro workstations with fast local SSDs in a Ceph storage pool. This &lt;a href="https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster">hyperconverged model&lt;/a> handles both block storage (VM disks) and file storage (ISOs) with configurable replication to survive the loss of any single host.&lt;/p>
&lt;p>More importantly, the micro workstations each consume only ~20W under normal operating conditions. Compared to a traditional rackmount server which runs around 200W at idle, I&amp;rsquo;m saving around 140W per hour while eliminating a major single point of failure in my infrastructure.&lt;/p>
&lt;p>It’s been a fun project seeing what this mini powerhouse can handle. I’m using it for everything from highly-available VMs to LXD containerized applications, and it’s holding up incredibly well. Ceph also has native CSI integrations for Kubernetes, which means any workloads I run in my &lt;a href="/posts/whats-in-my-lab/">Kubernetes cluster&lt;/a> can benefit from fast, fault-tolerant network storage.&lt;/p></content></item><item><title>Implementing a Private CA for the Home Lab</title><link>/posts/implementing-a-private-ca-for-home-use/</link><pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate><guid>/posts/implementing-a-private-ca-for-home-use/</guid><description>&lt;p>The low risk and low-to-no budget of a home lab environment often results in security taking a back seat. Services are sometimes left open and unguarded in the name of &amp;ldquo;Just Make It Work&amp;rdquo;. Home labs aside, the complexity of running even a halfway-decent security infrastructure makes doing so a non-starter even in many small business environments.&lt;/p>
&lt;p>As a result, the largest and most easily exploitable gap you&amp;rsquo;re bound to find in many home labs and small networks is unencrypted traffic. This of course allows for a variety of attack methods against locally-hosted services.&lt;/p></description><content>&lt;p>The low risk and low-to-no budget of a home lab environment often results in security taking a back seat. Services are sometimes left open and unguarded in the name of &amp;ldquo;Just Make It Work&amp;rdquo;. Home labs aside, the complexity of running even a halfway-decent security infrastructure makes doing so a non-starter even in many small business environments.&lt;/p>
&lt;p>As a result, the largest and most easily exploitable gap you&amp;rsquo;re bound to find in many home labs and small networks is unencrypted traffic. This of course allows for a variety of attack methods against locally-hosted services.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/unencrypted.png" />
&lt;figcaption class="center" >A common private network pitfall - unencrypted traffic and implicit trust&lt;/figcaption>
&lt;/figure>
&lt;p>Let&amp;rsquo;s focus on the two most common attacks - sniffing, where a malicious entity with access to your network devices mirrors packets to their own system to inspect the contents; and Man-in-the-Middle (MitM), where the attacker actually intercepts a traffic flow and impersonates the service on the other end.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/attacks.png" />
&lt;figcaption class="center" >Potential attack vectors - packet sniffing and Man-in-the-Middle&lt;/figcaption>
&lt;/figure>
&lt;p>These are two of the most common attacks around because they&amp;rsquo;re easy to implement and difficult to detect. Fortunately, they are also the easiest to mitigate. First, enforce encryption on the traffic to prevent a sniffing attack; and second, have your services provide a signed certificate to prove that they are who they say they are.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/encrypted.png" />
&lt;figcaption class="center" >The solution - protect traffic with HTTPS backed by a trusted certificate hierarchy&lt;/figcaption>
&lt;/figure>
&lt;p>There are almost as many examples available on the web for enabling HTTPS on a web server as there are web servers themselves. Rather than rehash that process, this post will focus on setting up a &lt;strong>Public Key Infrastructure&lt;/strong> (PKI) to facilitate issuing certificates to services on a local network and provide verification and revocation processes to maintain the integrity of your certificate hierarchy.&lt;/p>
&lt;h2 id="step-1---create-a-root-certificate">Step 1 - Create a Root Certificate&lt;/h2>
&lt;p>All PKIs start with a single self-signed certificate. Because this certificate sits at the top of the hierarchy, it is called a &lt;strong>Root Certification Authority&lt;/strong> or Root CA. Regardless of the size or scope of an organization, a Root CA is nothing more than a single X.509 certificate and private key. The private key is used to sign other certificates, and the certificate carries a fingerprint that uniquely identifies it as being linked to the private key.&lt;/p>
&lt;p>While I could go into great detail showing you how to use the OpenSSL command line to generate all the certificates, that would be a waste of my precious bandwidth. The Google search results for &amp;ldquo;OpenSSL Root CA&amp;rdquo; are littered with hundreds of guides written by people much smarter and more eloquent than me. And since working on a command line is a major part of my profession, I&amp;rsquo;ve come to appreciate a good GUI when one is available.&lt;/p>
&lt;p>For creating and managing a Root CA, I encourage you to check out a fantastic multi-platform application called &lt;a href="https://hohnstaedt.de/xca/">XCA&lt;/a>.&lt;/p>
&lt;p>XCA handles almost every function of the X.509 certificate lifecycle, including creating private keys, generating certificate requests, and signing requests as a self-signed certificate or using an existing certificate key pair stored in XCA&amp;rsquo;s database. I use XCA to manage my Root CA certificate and key, both of which are safely stored in an encrypted offline SQLite database.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/xca_private_key.png" />
&lt;figcaption class="center" >Creating a new EC private key in XCA&lt;/figcaption>
&lt;/figure>
&lt;p>In XCA, I created a private key for my Root CA using the P-521 Elliptic Curve. 521 bits may be absolute overkill for a small home lab environment, but the upside of having a hierarchy is that you will rarely ever be signing things with the Root CA, to the point where the performance impact of choosing P-521 over P-256 is insignificant.&lt;/p>
&lt;p>To create the Root CA, we must also create a Certificate Signing Request. The CSR is signed by the private key generated above, and the resulting certificate will carry the private key&amp;rsquo;s unique fingerprint.&lt;/p>
&lt;p>One of the most helpful features of XCA that I&amp;rsquo;ve come to appreciate is the ability to use templates to generate a certificate signing request. XCA ships with a generic CA template that you can select to generate a functional CA with minimal effort.&lt;/p>
&lt;p>On the Subject tab of the Certificate creation wizard, you&amp;rsquo;re presented with a number of fields that identify who or what the certificate represents. All of these fields are technically optional, but at the very least, you should set a descriptive Common Name to identify your Root CA certificate in browsers and other applications.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/root_ca_1.png" />
&lt;figcaption class="center" >Fill out as much or as little as you'd like - it's your CA!&lt;/figcaption>
&lt;/figure>
&lt;p>On the Extensions tab, you&amp;rsquo;ll set important details like Path Length and the certificate&amp;rsquo;s lifespan. Since this is a Root CA, you should select a reasonably long lifetime. Your intermediate certificates will have shorter lifespans because they&amp;rsquo;ll be used more frequently, and as long as the Root CA is valid, you can always generate another intermediate.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/root_ca_2.png" />
&lt;figcaption class="center" >100 years? Why not - you're in this for the long haul.&lt;/figcaption>
&lt;/figure>
&lt;p>The other fields on this page are not relevant for a Root CA, but will be necessary when creating Intermediates or End Entity certificates if you choose to publish a Certificate Revocation List or OCSP endpoint.&lt;/p>
&lt;p>Once you click OK and sign your certificate, you have a functioning Root CA. You can add this certificate to the trusted certificate store on your systems, and any certificate signed by the Root CA will be inherently trusted as well.&lt;/p>
&lt;p>&lt;strong>Last but not least,&lt;/strong> do not forget to encrypt your XCA database or export the Root CA certificate and key with a strong password. If you lose the key, you lose the ability to sign anything with the Root CA.&lt;/p>
&lt;h2 id="step-2---create-an-intermediate-ca">Step 2 - Create an Intermediate CA&lt;/h2>
&lt;p>In a typical PKI heirarchy, the Root CA never directly signs an &amp;ldquo;End Entity&amp;rdquo; certificate such as a webserver or mTLS client certificate. An &lt;strong>Intermediate CA&lt;/strong> is a certificate that receives its authority from the Root CA, and signs certificates for other subordinate CAs or End Entities directly.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/chain_of_trust.png" />
&lt;/figure>
&lt;p>The process to create an Intermediate CA is almost identical to a Root CA, with the following exceptions:&lt;/p>
&lt;ol>
&lt;li>The Intermediate CA must be signed by the Root CA&amp;rsquo;s private key, giving it a unique cryptographic signature that can be independently verified against the Root CA&amp;rsquo;s public certificate.&lt;/li>
&lt;li>The Intermediate CA usually has a much shorter lifespan than the Root CA, typically on the order of 5-10 years. An Intermediate CA will sign a large number of certificates, and should therefore be recycled more frequently.&lt;/li>
&lt;li>The Intermediate CA certificate should list a &lt;strong>CRL Distribution Point&lt;/strong> - a URL where the Root CA&amp;rsquo;s Certificate Revocation List is published - so clients can confirm that the Root CA has not revoked its signing of the Intermediate CA.&lt;/li>
&lt;li>The Intermediate CA should also manage its own CRL and provide its CRL Distribution Point on all the certificates that it signs.&lt;/li>
&lt;/ol>
&lt;p>Because the Intermediate CA will be used much more frequently, its functions are usually handled by purpose-built server software. There are a number of platforms available - and to be honest, this is where I had the most difficulty. Solutions such as Active Directory Certificate Services, PrimeKey EJBCA, and OpenXPKI are popular for large-scale PKI deployments, but that enterprise-ready approach means they&amp;rsquo;re not well-suited for a minimal deployment, and &amp;ldquo;ease of use&amp;rdquo; is sorely lacking.&lt;/p>
&lt;p>For seamless integration into my existing AD environment, Active Directory Certificate Services was the most logical choice. Regardless of the platform you choose for your intermediate, the process for signing it from your Root CA is mostly unchanged. You&amp;rsquo;ll still be able to generate a request, import it into your Root CA&amp;rsquo;s XCA database to be signed by the Root CA private key, and then export out the generated certificate.&lt;/p>
&lt;h2 id="step-3---create-an-issuing-ca">Step 3 - Create an Issuing CA&lt;/h2>
&lt;p>With a working Root and Intermediate CA in my chain of trust, I wanted to add one final layer of authority to delegate daily signing responsibilities to. For that, I decided to give &lt;a href="https://smallstep.com/certificates/">Smallstep&amp;rsquo;s open-source CA software&lt;/a> a try.&lt;/p>
&lt;p>I originally evaluated Smallstep early on when searching for a product to serve as my Intermediate CA. I didn&amp;rsquo;t consider Step CA to be a serious contender because of its lack of management features and strong emphasis on automation. What eventually won me over when I evaluated it as a potential Issuing CA was its ease of setup and ACME support out of the box.&lt;/p>
&lt;p>&lt;strong>Pros&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Simple command-line setup&lt;/strong> - Smallstep has a number of very useful tutorials, including how to initialize a Step CA as a non-Root authority. For that, you need to create the request using Step and sign it using one of your other certificates.&lt;/li>
&lt;li>&lt;strong>Native ACME support&lt;/strong> - The &lt;code>step-ca&lt;/code> software can be set up to run as a system daemon, and provides a built-in webserver and API to handle certificate requests from clients. Using the provider functionality, it can also be configured as an ACME server similar to LetsEncrypt. This allows me to have a custom, internal-only ACME CA for my local network!&lt;/li>
&lt;li>&lt;strong>Step client daemon mode&lt;/strong> - The &lt;code>step-ca&lt;/code> software allows any valid certificate to renew itself with the CA - no manual renewal or admin intervention required. The &lt;code>step&lt;/code> CLI features a &amp;ldquo;Daemon mode&amp;rdquo; that can run in the background and automatically renew a given certificate when it gets close to expiration.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Cons&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>No GUI&lt;/strong> - If you like having a web interface to view your certificates, Smallstep does allow linking a single &lt;code>step-ca&lt;/code> their Smallstep cloud service. I didn&amp;rsquo;t have the greatest experience managing my CA once I did this, because it meant that some settings in the &lt;code>step-ca&lt;/code> config file were no longer honored and everything had to be managed through their cloud service with OIDC authentication. I rolled the change back and am instead running &lt;code>step-ca&lt;/code> strictly locally.&lt;/li>
&lt;li>&lt;strong>Highly Opinionated Defaults&lt;/strong> - The &lt;code>step&lt;/code> CLI&amp;rsquo;s certificate request function seems to abstract away a lot of otherwise useful X.509 extensions and features. You can still sign a traditional CSR file using the &lt;code>step-ca&lt;/code>, so I still prefer to generate the key and CSR myself to ensure the resulting certificate works for my needs.&lt;/li>
&lt;/ul>
&lt;p>I was very skeptical of Smallstep&amp;rsquo;s product at first, but once I found the right use case for their solution, I was very impressed with the result. I followed their official tutorials on &lt;a href="https://smallstep.com/docs/tutorials/intermediate-ca-new-ca">setting up a new Intermediate CA from an existing Root CA&lt;/a> and &lt;a href="https://smallstep.com/docs/step-ca/certificate-authority-server-production/#running-step-ca-as-a-daemon">running step-ca as a daemon&lt;/a> and was quickly deploying certificates to several servers on my network.&lt;/p>
&lt;p>I even created a &lt;a href="https://gist.github.com/danclough/ee0b25c285cc2fc280e5d2d3a6f855d7">template for a systemd unit file&lt;/a> to run the &lt;code>step&lt;/code> CLI in daemon mode, watching and renewing certificates automatically when they come close to expiring.&lt;/p>
&lt;h2 id="the-final-product">The Final Product&lt;/h2>
&lt;p>The end result of my home lab PKI experiment looks like this:&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/hierarchy.png" />
&lt;/figure>
&lt;p>All trust originates with the Root CA, an offline ECC certificate stored in an encrypted database.&lt;/p>
&lt;p>The Root CA signs the Intermediate CA managed by Active Directory Certificate Services, which hosts CRLs and an OCSP service for both the Intermediate and Root CA.&lt;/p>
&lt;p>The Intermediate CA signs the Smallstep CA, which provides CLI-based certificate enrollment and ACME services for local servers and services.&lt;/p>
&lt;p>The Smallstep CA signs dozens of certificates for the various services I run locally, whether issued through the &lt;code>step&lt;/code> CLI or through the native ACME API endpoint. With Smallstep&amp;rsquo;s emphasis on automated issuance and renewals, I&amp;rsquo;ve been able to achieve unbelievably short certificate lifespans of just 24 hours.&lt;/p>
&lt;p>It should be noted that I went through a number of iterations before landing on my current architecture. I spent multiple days researching the array of different PKI products out there, and at least a week wrestling with the setup and configuration of other tools like EJBCA that I ended up scrapping for simplicity&amp;rsquo;s sake. It&amp;rsquo;s perfectly acceptable in a small environment to create a two-tiered hierarchy - for instance, cutting out AD and running a CA hierarchy solely comprised of XCA and Smallstep.&lt;/p>
&lt;p>In the end, what matters most is that you&amp;rsquo;ve got at least some foundation to build upon, no matter how simple or complex it may be.&lt;/p></content></item><item><title>The Case for Home Lab Security</title><link>/posts/case-for-homelab-security/</link><pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate><guid>/posts/case-for-homelab-security/</guid><description>&lt;p>Perhaps unsurprisingly, as my home lab and local area network have matured over the years, both I and my family have come to depend on the assortment of services that I run strictly within the four walls of our home. Knowing that our data is &lt;em>physically&lt;/em> secure, we often tend to take other forms of security for granted.&lt;/p></description><content>&lt;p>Perhaps unsurprisingly, as my home lab and local area network have matured over the years, both I and my family have come to depend on the assortment of services that I run strictly within the four walls of our home. Knowing that our data is &lt;em>physically&lt;/em> secure, we often tend to take other forms of security for granted.&lt;/p>
&lt;p>For instance, we put all of our important files - recipes, documents, photos, home videos, etc. - on &amp;ldquo;the server&amp;rdquo;. The contents of said server are invaluable to us, economically and emotionally. I take very specific steps to ensure the safety of our data, including ZFS volume-level encryption and frequent encrypted backups to an off-site backup provider. (Shoutout to &lt;a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2&lt;/a>!)&lt;/p>
&lt;p>Having reliable, 24x7 access to all of my family&amp;rsquo;s data is one hell of an accomplishment, but it&amp;rsquo;s not enough any more. I&amp;rsquo;ve seen too many software vulnerabilities in my still-young IT career to be able to blindly trust any single layer of application security. For the same reasons that my data needs to remain safe and accessible, I also need to keep it secure from prying eyes and malicious actors.&lt;/p>
&lt;p>The IT Infosec landscape of today is almost unrecognizable compared to when I started in IT just over 10 years ago. Back then, vendors were clawing their way into our voicemails, inboxes, and conference rooms to pitch some all-encompassing enterprise security platform that promised to keep mysterious hooded hackers out of your unauthenticated SMB 1 file shares and publicly-accessible SharePoint sites.&lt;/p>
&lt;p>Today, the vendors are still there and eager as ever to give you a four-hour sales demo - but as an industry we&amp;rsquo;ve also adopted a lot of common-sense policies that thankfully don&amp;rsquo;t take a multi-million-dollar contract and six figures worth of computing resources to maintain. Namely:&lt;/p>
&lt;ul>
&lt;li>Password-protect everything&lt;/li>
&lt;li>Enable Two-Factor Authentication (2FA)&lt;/li>
&lt;li>Configure Single Sign-On (SSO) wherever possible&lt;/li>
&lt;li>Perhaps most importantly, HTTPS &lt;strong>absolutely everywhere&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>If you follow even just half of the policies above, I promise you - your lab will be just as secure, if not more secure, than the majority of corporate IT environments you&amp;rsquo;ll come across.&lt;/p>
&lt;p>That being said, here are the steps I&amp;rsquo;m taking to secure my home lab:&lt;/p>
&lt;ol>
&lt;li>Implement an internal Public Key Infrastructure (PKI) for my local domain,&lt;/li>
&lt;li>Secure all TLS-capable services on my local network with certificates from the internal PKI, and&lt;/li>
&lt;li>Setup auto-renewal to allow short certificate lifespans (days to weeks) without manual intervention or rotation.&lt;/li>
&lt;/ol>
&lt;p>I fully intend for this blog to be a place for learning and sharing my experiences, so in a coming post I&amp;rsquo;ll attempt to document every step of the process and write about a few of the hurdles I had to overcome. I&amp;rsquo;ll also share my thoughts on a few of the fantastic open-source products that I tried out along the way.&lt;/p></content></item><item><title>Kubernetes @ Home</title><link>/posts/kubernetes-at-home/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>/posts/kubernetes-at-home/</guid><description>&lt;p>Now that I&amp;rsquo;ve shared some of my physical infrastructure in my home lab, I want to share the service topology for the specific services I run at home for home automation, secure storage, and even this website.
&lt;figure class="center" >
&lt;img src="/images/2021/05/service-topology-1.png" />
&lt;/figure>
&lt;/p></description><content>&lt;p>Now that I&amp;rsquo;ve shared some of my physical infrastructure in my home lab, I want to share the service topology for the specific services I run at home for home automation, secure storage, and even this website.
&lt;figure class="center" >
&lt;img src="/images/2021/05/service-topology-1.png" />
&lt;/figure>
&lt;/p>
&lt;h2 id="services">Services&lt;/h2>
&lt;p>As you can see, a number of the specific services I run are focused around infrastructure and system management. Services such as Grafana and Kibana are essential for monitoring and observability for my entire home lab environment. A few of the other services not pictured, for brevity:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>phpIPAM&lt;/strong> - Network IP management and tracking database&lt;/li>
&lt;li>&lt;strong>LDAP Self-Service Portal&lt;/strong> - A simple self-service web application for managing LDAP/Active Directory passwords&lt;/li>
&lt;li>&lt;strong>phpMyAdmin&lt;/strong> - Web-based management interface for my external MariaDB/Galera database cluster&lt;/li>
&lt;/ul>
&lt;h2 id="storage">Storage&lt;/h2>
&lt;p>One recent addition to my lab environment is &lt;a href="https://ceph.io/">Ceph&lt;/a>.&lt;/p>
&lt;p>One of the most difficult and frustrating problems of managing containerized applications across multiple hosts is data persistence and replication - especially when operating on bare metal. In my professional experience managing platforms such as &lt;em>Azure Kubernetes Service&lt;/em>, I&amp;rsquo;ve been able to leverage the built-in StorageClass drivers such as &lt;code>AzureFile&lt;/code> and &lt;em>Azure Managed Disks&lt;/em> for each managed Kubernetes environment.&lt;/p>
&lt;p>When you&amp;rsquo;re on bare metal, however, you&amp;rsquo;re left to fend for yourself. I found Ceph much more complicated to configure at first than GlusterFS, but more resilient and overall easier to integrate into my K8s environment.&lt;/p>
&lt;h2 id="ingress-and-load-balancing">Ingress and Load Balancing&lt;/h2>
&lt;p>I&amp;rsquo;ve been using Traefik as an Ingress Controller for Docker for several years, and it&amp;rsquo;s greatly simplified the ingress routing configuration of each of my services.&lt;/p>
&lt;p>Traefik has built-in ACME support which allows me to utilize Let&amp;rsquo;s Encrypt for solutions that I host on an external domain (i.e., this website and my NextCloud instance). For services on my internal LAN domain, I employ &lt;a href="https://cert-manager.io/">cert-manager&lt;/a> to automatically generate and manage TLS certificates from an internal Certificate Authority based on certificate requests from any workloads deployed to my cluster.&lt;/p>
&lt;p>Similarly to storage, another hard problem of bare-metal K8s is the lack of a network-level load balancer to bring ingress traffic to your cluster. K8s doesn&amp;rsquo;t have any integrated functionality to provide a cluster-wide, externally-accessible IP address for Services. The most effective solution to this problem that I&amp;rsquo;ve found thus far is &lt;a href="https://metallb.universe.tf/">MetalLB&lt;/a>. MetalLB interacts with your external network infrastructure - whether through Layer 2 ARP or Layer 3 routing protocols - to provide Services with an external IP address through the existing LoadBalancer Service type.&lt;/p></content></item><item><title>Migrating Kubernetes from Docker to containerd</title><link>/posts/from-docker-to-containerd/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>/posts/from-docker-to-containerd/</guid><description>&lt;p>On December 2nd, a &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">surprise announcement&lt;/a> made waves in the Kubernetes Twitter-sphere - that after the upcoming 1.20 release, Docker would be officially deprecated.&lt;/p>
&lt;h3 id="oh-no">Oh no!&lt;/h3>
&lt;p>Due to widespread confusion over what &amp;ldquo;Docker&amp;rdquo; means in specific contexts, many people panicked - myself included. Due to its popularity and ease of use, the Docker engine has become synonymous with &amp;ldquo;containers&amp;rdquo;. However, Docker is really an entire ecosystem of container tools and processes, including building and shipping container images. So what does this announcement mean, and what are the implications for everyone using it?&lt;/p></description><content>&lt;p>On December 2nd, a &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">surprise announcement&lt;/a> made waves in the Kubernetes Twitter-sphere - that after the upcoming 1.20 release, Docker would be officially deprecated.&lt;/p>
&lt;h3 id="oh-no">Oh no!&lt;/h3>
&lt;p>Due to widespread confusion over what &amp;ldquo;Docker&amp;rdquo; means in specific contexts, many people panicked - myself included. Due to its popularity and ease of use, the Docker engine has become synonymous with &amp;ldquo;containers&amp;rdquo;. However, Docker is really an entire ecosystem of container tools and processes, including building and shipping container images. So what does this announcement mean, and what are the implications for everyone using it?&lt;/p>
&lt;p>The only thing Kubernetes is deprecating is using Docker as a container runtime, and the reasoning is sound. Docker&amp;rsquo;s lack of support for the &amp;ldquo;Container Runtime Interface&amp;rdquo; API - or CRI, for short - forced Kubernetes to implement an abstraction layer called &amp;ldquo;dockershim&amp;rdquo; to allow Kubernetes to manage containers in Docker. The burden of maintaining dockershim was too great to bear, so they are deprecating dockershim in release 1.20, and will eventually remove it entirely in 1.22.&lt;/p>
&lt;p>There are two other container runtimes featured in the Kubernetes quickstart guide as an alternative to Docker - &lt;code>containerd&lt;/code> and CRI-O. &lt;code>containerd&lt;/code> is the same runtime that Docker itself uses internally, just without the fancy Docker wrapping paper and tools.&lt;/p>
&lt;h3 id="ugh">Ugh.&lt;/h3>
&lt;p>Annoyingly enough, I had recently finished migrating my entire homelab container infrastructure to Kubernetes three months ago, with Docker as the container runtime. I initially thought, &amp;ldquo;Crap. Guess I&amp;rsquo;ll be rebuilding my cluster!&amp;rdquo; Then I began to think about what such a change would look like, and whether replacing Docker with &lt;code>containerd&lt;/code> in the same cluster is doable.&lt;/p>
&lt;h3 id="hmm">Hmm&amp;hellip;&lt;/h3>
&lt;p>Turns out, it is!&lt;/p>
&lt;p>I have a 3-node HA cluster which I created using kubeadm. Because I have multiple control plane nodes, I can remove them one at a time using &lt;code>kubeadm reset&lt;/code>, rebuild them with &lt;code>containerd&lt;/code> instead of Docker, and then rejoin using &lt;code>kubeadm join&lt;/code>.&lt;/p>
&lt;p>Here are the steps I came up with:&lt;/p>
&lt;h3 id="uninstalling-docker">Uninstalling Docker&lt;/h3>
&lt;ol>
&lt;li>Using &lt;code>kubectl&lt;/code>, drain and evict pods from the target node.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl drain &lt;span style="color:#e6db74">${&lt;/span>node&lt;span style="color:#e6db74">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>On the target node, use &lt;code>kubeadm&lt;/code> to remove the node from the cluster.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubeadm reset
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>Once &lt;code>kubeadm reset&lt;/code> is finished, stop Docker and finish cleaning up the node.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemctl stop docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /etc/cni/net.d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>iptables --flush
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>Uninstall the Docker CE suite and CLI.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apt-get -y remove docker-ce*
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /var/lib/docker/*
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /var/lib/dockershim
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="5">
&lt;li>Now&amp;rsquo;s a great time to update your kernel and OS packages&amp;hellip;&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apt-get update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apt-get -y dist-upgrade
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="6">
&lt;li>&amp;hellip;and reboot!&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>shutdown -r now
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="installing-containerd">Installing containerd&lt;/h3>
&lt;p>(These steps are lifted straight from the fantastic &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd">k8s containerd docs&lt;/a>!)&lt;/p>
&lt;ol start="7">
&lt;li>Apply the module configs for &lt;code>containerd&lt;/code>&amp;rsquo;s required kernel modules.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">overlay
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">br_netfilter
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo modprobe overlay
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo modprobe br_netfilter
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="8">
&lt;li>Set sysctl tuning parameters for Kubernetes CRI&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">net.ipv4.ip_forward = 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sysctl --system
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="9">
&lt;li>Install &lt;code>containerd&lt;/code> if not already installed&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apt-get install -y containerd.io
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h5 id="a-note-on-filesystems">A note on filesystems&lt;/h5>
&lt;p>Since these nodes were running Docker, all of the container data is stored in /var/lib/docker. With &lt;code>containerd&lt;/code>, container data is now stored in /var/lib/containerd. If you had the Docker data directory on its own filesystem, you&amp;rsquo;ll need to remove it and create one for &lt;code>containerd&lt;/code>. The exact steps depend in your system, so I won&amp;rsquo;t include them here.&lt;/p>
&lt;h5 id="now-back-to-the-fun">Now back to the fun!&lt;/h5>
&lt;hr>
&lt;ol start="10">
&lt;li>Generate a default configuration:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mkdir -p /etc/containerd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>containerd config default &amp;gt; /etc/containerd/config.toml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="11">
&lt;li>Modify the &lt;code>config.toml&lt;/code> file generated above to enable the systemd cgroup driver:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#a6e22e">plugins&lt;/span>.&lt;span style="color:#e6db74">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.&lt;span style="color:#a6e22e">containerd&lt;/span>.&lt;span style="color:#a6e22e">runtimes&lt;/span>.&lt;span style="color:#a6e22e">runc&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#a6e22e">plugins&lt;/span>.&lt;span style="color:#e6db74">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.&lt;span style="color:#a6e22e">containerd&lt;/span>.&lt;span style="color:#a6e22e">runtimes&lt;/span>.&lt;span style="color:#a6e22e">runc&lt;/span>.&lt;span style="color:#a6e22e">options&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SystemdCgroup&lt;/span> = &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="12">
&lt;li>Now enable and start &lt;code>containerd&lt;/code>!&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemctl enable --now containerd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="13">
&lt;li>Make sure &lt;code>containerd&lt;/code> is happy before proceeding.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemctl status containerd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your system is now fully configured with the &lt;code>containerd&lt;/code> runtime - but before we rejoin the cluster, there&amp;rsquo;s one more step to get kubeadm to play nicely with it!&lt;/p>
&lt;h3 id="updating-the-kubelet-configuration">Updating the kubelet configuration&lt;/h3>
&lt;p>Since this cluster was originally built with the Docker runtime, the default kubelet configuration does not explicitly set a cgroup driver. By default, kubeadm with Docker auto-detects the cgroup driver - but other runtimes like &lt;code>containerd&lt;/code> don&amp;rsquo;t support that yet. As a result, when you &lt;code>kubeadm join&lt;/code> a &lt;code>containerd&lt;/code> node without a cgroup driver specified, the kubelet won&amp;rsquo;t start. You can ninja-edit the &lt;code>/var/lib/kubelet/config.yaml&lt;/code> file when joining and then restart the kubelet, but that&amp;rsquo;s tedious and unnecessary.&lt;/p>
&lt;p>Fortunately, we can update the baseline kubelet config at the cluster level to specify the right cgroup driver to use.&lt;/p>
&lt;ol start="14">
&lt;li>Edit the baseline kubelet config for your Kubernetes version - 1.18, 1.19, etc.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl edit cm -n kube-system kubelet-config-1.18
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="15">
&lt;li>Add the following entry for &lt;code>cgroupDriver&lt;/code>:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">data&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubelet&lt;/span>: |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> cgroupDriver: systemd
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="joining-the-cluster">Joining the cluster&lt;/h3>
&lt;ol start="16">
&lt;li>Proceed to &lt;code>kubeadm join&lt;/code> your node with the appropriate kubeadm command! You can run &lt;code>kubeadm token create --print-join-command&lt;/code> to create a new token.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubeadm join 123.45.67.89:6443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --token &amp;lt;...snip...&amp;gt; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --discovery-token-ca-cert-hash sha256:&amp;lt;...snip...&amp;gt; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">[&lt;/span>--control-plane --certificate-key &amp;lt;...snip...&amp;gt;&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For control plane nodes, be sure to include the &lt;code>--control-plane&lt;/code> flag and &lt;code>--certificate-key&lt;/code> for your cluster - otherwise the node will join as a worker! I made this mistake and had to re-reset and rejoin the first node I converted. Use &lt;code>kubeadm init phase upload-certs --upload-certs&lt;/code> on another control plane node to reupload your certificates to the cluster, and then pass the provided certificate key to &lt;code>kubeadm join&lt;/code>.&lt;/p>
&lt;h3 id="clean-up">Clean-up&lt;/h3>
&lt;p>Once your new node is joined, wait a few minutes for your CNI plugin to reprovision the networking stack. Once you&amp;rsquo;re satisfied and the node shows &lt;code>Ready&lt;/code> in &lt;code>kubectl get nodes&lt;/code>, you can uncordon the node with &lt;code>kubectl uncordon&lt;/code>.&lt;/p>
&lt;p>And finally, if necessary, don&amp;rsquo;t forget to re-taint your new control plane node! When &lt;code>kubeadm&lt;/code> rejoins the node, it applies the same default restriction to prevent control plane nodes from running worker pods. I find separate control planes unnecessary for my homelab, so I taint them to allow pods to run anywhere.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="final-thoughts">Final thoughts&lt;/h3>
&lt;p>Now, granted - this process is &lt;em>extremely&lt;/em> unnecessary, and runs contrary to the cloud ethos that nodes should be treated like cattle. But for someone running a small bare-metal environment - where provisioning new nodes &lt;em>isn&amp;rsquo;t&lt;/em> entirely automated - these steps save a lot of time otherwise spent rebuilding VMs from the ground up, assigning IP addresses, updating DNS, and potentially building a whole new cluster.&lt;/p>
&lt;p>And as an &lt;em>added&lt;/em> bonus, I now know more about Kubernetes and container runtimes than I did last week.&lt;/p></content></item><item><title>What's In My Lab</title><link>/posts/whats-in-my-lab/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>/posts/whats-in-my-lab/</guid><description>&lt;p>Like many people in IT, I&amp;rsquo;ve been running a home lab for several years. My home lab has become progressively more complicated over the years as I&amp;rsquo;ve layered in new technologies that I want to explore and added new services to my home network.&lt;/p></description><content>&lt;p>Like many people in IT, I&amp;rsquo;ve been running a home lab for several years. My home lab has become progressively more complicated over the years as I&amp;rsquo;ve layered in new technologies that I want to explore and added new services to my home network.&lt;/p>
&lt;h3 id="hardware">Hardware&lt;/h3>
&lt;p>My home lab runs mainly on low-power Dell OptiPlex ultra-small-form-factor (USFF) hardware. I wanted this lab to be always-on - so small, quiet, and energy-efficient hardware is a must-have. The notable exception is my NAS, a 2U Dell PowerEdge rackmount server with an array of 3.5&amp;quot; SAS spinning disks striped in a large ZFS RAID-Z2 pool.&lt;/p>
&lt;h3 id="hypervisors">Hypervisors&lt;/h3>
&lt;p>For several years, I ran VMware ESXi mostly as a learning opportunity. Over time I found that the overhead of running ESXi on low-spec commodity hardware was too great, and decided to shift my workloads over to &lt;a href="https://www.proxmox.com/">Proxmox VE&lt;/a>.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2020/10/infra_services.png" />
&lt;/figure>
&lt;h3 id="services">Services&lt;/h3>
&lt;p>Each physical system in my homelab runs the Debian-based Proxmox VE hypervisor. The hypervisors host a mix of LXC-based containers and QEMU virtual machines. From top to bottom:&lt;/p>
&lt;ul>
&lt;li>HAProxy
&lt;ul>
&lt;li>Two very lightweight LXD containers serving as a virtual network load balancer&lt;/li>
&lt;li>HAProxy acts as a TCP and HTTP load balancer&lt;/li>
&lt;li>Keepalived manages virtual IPs that failover between the two containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MariaDB Galera Cluster
&lt;ul>
&lt;li>Three Debian LXD containers running MariaDB with Galera multi-master replication&lt;/li>
&lt;li>Clients access the MariaDB cluster using a VIP and server pool managed by the HAProxy cluster&lt;/li>
&lt;li>I wrote a &lt;a href="https://github.com/danclough/mysql-healthcheck">monitoring daemon in Go&lt;/a> that provides HTTP healthcheck capabilities for HAProxy to determine the health of each MariaDB instance&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ceph
&lt;ul>
&lt;li>Each Proxmox host runs a Ceph monitor&lt;/li>
&lt;li>Each host also hosts a Ceph OSD on an internal 2.5&amp;quot; SATA disk&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Active Directory
&lt;ul>
&lt;li>Three MSDN Server 2019 Core VMs across each host provide Active Directory services&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kubernetes
&lt;ul>
&lt;li>3 control plane nodes with taints&lt;/li>
&lt;li>1 worker node&lt;/li>
&lt;li>Workloads on Kubernetes are able to leverage the external MariaDB and Ceph clusters for persistence&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In addition to these clustered services, I also deployed a few standalone services for better management and visibility across my environment:&lt;/p>
&lt;ul>
&lt;li>Chef Infra Server
&lt;ul>
&lt;li>Manages automation for VMs, LXD containers, and physical hosts&lt;/li>
&lt;li>I&amp;rsquo;ve also open-sourced &lt;a href="https://github.com/danclough/chef-qemu_guest">a few of the cookbooks&lt;/a> I created to manage my lab systems&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Elastic ELK Stack
&lt;ul>
&lt;li>Logstash takes syslog data and Kubernetes logs from Filebeat and indexes them into Elasticsearch&lt;/li>
&lt;li>A Kibana instance running on Kubernetes provides search and visualization capabilities&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>InfluxDB
&lt;ul>
&lt;li>Aggregates time-series monitoring data from telegraf clients on Linux&lt;/li>
&lt;li>Provides data to a Grafana instance running on Kubernetes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>Designing a Homelab</title><link>/posts/designing-a-homelab/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>/posts/designing-a-homelab/</guid><description>&lt;p>Just as a motorhead might get their endorphine fix attending a vintage car show, I browse &lt;a href="https://reddit.com/r/homelab">r/homelab&lt;/a> almost daily and fawn over the beautifully-curated racks full of enterprise grade hardware sitting in some random person&amp;rsquo;s apartment. I often get the feeling that I won&amp;rsquo;t be truly satisfied until I have a few racks of my own, full of routers, switches, servers, and disk shelves.&lt;/p>
&lt;p>What would I do with them? Aside from stress-testing my home&amp;rsquo;s electrical wiring, I really don&amp;rsquo;t know.&lt;/p></description><content>&lt;p>Just as a motorhead might get their endorphine fix attending a vintage car show, I browse &lt;a href="https://reddit.com/r/homelab">r/homelab&lt;/a> almost daily and fawn over the beautifully-curated racks full of enterprise grade hardware sitting in some random person&amp;rsquo;s apartment. I often get the feeling that I won&amp;rsquo;t be truly satisfied until I have a few racks of my own, full of routers, switches, servers, and disk shelves.&lt;/p>
&lt;p>What would I do with them? Aside from stress-testing my home&amp;rsquo;s electrical wiring, I really don&amp;rsquo;t know.&lt;/p>
&lt;p>And with that, we have our first piece of advice for beginning homelabbers&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Don&amp;rsquo;t Overthink It&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>Browsing &lt;a href="https://reddit.com/r/homelab">r/homelab&lt;/a> is a blessing and a curse. You want the shinys and the blinkenlights, but beauty is only skin deep.&lt;/p>
&lt;ol start="2">
&lt;li>&lt;strong>Be Goal-Oriented&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>What do you want or need from your first lab? Consider the examples below and good, inexpensive options for each.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Designing web sites&lt;/strong> - static pages, PHP, Node.JS, etc.
&lt;ul>
&lt;li>Raspberry Pi 3 or 4 - $40&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Small low-power media server&lt;/strong> - Plex, XBMC, etc.
&lt;ul>
&lt;li>Raspberry Pi 4 (4GB) with a USB hard drive - $80&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Working with containers&lt;/strong> - Docker, Docker Compose
&lt;ul>
&lt;li>Raspberry Pi 4 (4-8GB) with a USB hard drive or SSD - $140
&lt;ul>
&lt;li>The 64-bit arm64 architecture is supported by many popular containers on Docker Hub and other public registries.&lt;/li>
&lt;li>RPi 4 supports booting from USB devices rather than unreliable MicroSD, alleviating a common complaint of Pi users with write-heavy workloads.&lt;/li>
&lt;li>The newest Pi 4 SKU comes with 8GB RAM - plenty of headroom for running multiple containers.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Small NUC-sized x86-64 PC - $200&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Running VMs&lt;/strong>
&lt;ul>
&lt;li>Any x86-64 PC running Proxmox VE, a free Linux QEMU-based hypervisor&lt;/li>
&lt;li>Proxmox VE&amp;rsquo;s hardware support is only limited by what Debian supports - which, as it turns out, is &lt;em>a lot&lt;/em>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>&lt;strong>Don&amp;rsquo;t Be Afraid To Break It&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>As a hands-on learner, I retain information most effectively when I can apply it to a real problem. As you change up, tear down, and rebuild the parts of your homelab, you might find a new solution to an old problem, or think up a new way of doing something you did before.&lt;/p>
&lt;p>The beauty of a homelab is all right there in the name - &lt;strong>home&lt;/strong> &lt;strong>lab&lt;/strong>. It&amp;rsquo;s yours to break and fix as you see fit, for learning or just for fun. No service-level agreements to worry about, no stakeholders to notify, no change control processes to adhere to.&lt;/p>
&lt;p>If you come to depend on the services you run in your homelab, start focusing your efforts on high availability. A few examples to note:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Failover&lt;/strong> - Set up a load balancer to have one IP or web URL point to two servers, so when one goes down the other takes over. Often this happens without you even knowing, so set up monitoring while you&amp;rsquo;re at it to know when a server experiences issues.&lt;/li>
&lt;li>&lt;strong>Fault Tolerance -&lt;/strong> Are all your files stored on one hard drive? Try setting up a RAID mirror so your services keep running if a drive dies. Do you frequently experience power outages? Install an &lt;strong>Uninterruptable Power Supply (UPS)&lt;/strong> to provide backup power to your devices during a utility blackout.&lt;/li>
&lt;li>&lt;strong>Disaster Recovery -&lt;/strong> How quickly can you get things back up and running after a server failure? **** Copy your services or container configs to another server using a scheduled backup process, and test the restore process periodically to ensure that it works.&lt;/li>
&lt;/ul></content></item><item><title>The Importance of Homelabs</title><link>/posts/importance-of-homelabs/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>/posts/importance-of-homelabs/</guid><description>&lt;p>&amp;ldquo;Was that in the notes?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;Where&amp;rsquo;d you learn that?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;How do you just &lt;em>know&lt;/em> that?&amp;rdquo;&lt;/p>
&lt;p>These are all questions I&amp;rsquo;ve been asked, in one form or another, over the last 10 years of working in IT. The situations vary, but there&amp;rsquo;s a common thread to each - an issue identified, a problem solved, a better way to complete some task.&lt;/p>
&lt;p>Most of the time, I don&amp;rsquo;t answer. How do you explain that you knew that &lt;em>one specific thing&lt;/em> because of the three hours you spent last month sitting in your cold basement, begging your production NAS to start back up after an upgrade? Well, it&amp;rsquo;s not &lt;em>production, per se&lt;/em>&amp;hellip; but to your family or roommates, it may as well be. &amp;ldquo;It&amp;rsquo;s a long story. I&amp;rsquo;ve seen it before. Not here - on my server at home.&amp;rdquo;&lt;/p></description><content>&lt;p>&amp;ldquo;Was that in the notes?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;Where&amp;rsquo;d you learn that?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;How do you just &lt;em>know&lt;/em> that?&amp;rdquo;&lt;/p>
&lt;p>These are all questions I&amp;rsquo;ve been asked, in one form or another, over the last 10 years of working in IT. The situations vary, but there&amp;rsquo;s a common thread to each - an issue identified, a problem solved, a better way to complete some task.&lt;/p>
&lt;p>Most of the time, I don&amp;rsquo;t answer. How do you explain that you knew that &lt;em>one specific thing&lt;/em> because of the three hours you spent last month sitting in your cold basement, begging your production NAS to start back up after an upgrade? Well, it&amp;rsquo;s not &lt;em>production, per se&lt;/em>&amp;hellip; but to your family or roommates, it may as well be. &amp;ldquo;It&amp;rsquo;s a long story. I&amp;rsquo;ve seen it before. Not here - on my server at home.&amp;rdquo;&lt;/p>
&lt;p>Engineering knowledge is a perishable resource. Its decay comes not just from continual technological advancement, but from your own experiential half-life as well. I haven&amp;rsquo;t touched a Cisco device in over a year and my last Cisco cert has since lapsed. I could still poke around in the IOS CLI with a little fumbling and an embarrassing overuse of the &lt;code>?&lt;/code>, but I doubt I could set up an eBGP neighbor on-demand without a reference page and a few minutes of review to jog my memory.&lt;/p>
&lt;p>Other times, it&amp;rsquo;s not even the knowledge itself that you benefit from. The information is out there - you&amp;rsquo;re free to access it as needed. The concepts __ are what you really need to succeed. You&amp;rsquo;ll have a much harder time accomplishing things with the reference pages and a few minutes of study when you never had the opportunity to explore routing and BGP concepts in the first place.&lt;/p>
&lt;p>Having a homelab is a quick and easy way to stand out as a professional. That statement holds true whether you&amp;rsquo;re a specialist or a generalist, a 10-year veteran or just starting out in Tech.&lt;/p></content></item></channel></rss>