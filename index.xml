<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Buffer Overflow</title><link>/</link><description>Recent content on Buffer Overflow</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>All original content licensed under &lt;a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0&lt;/a> unless noted otherwise.</copyright><lastBuildDate>Fri, 31 Dec 2021 14:34:28 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Ampere Altra vs Raspberry Pi 4</title><link>/posts/ampere-altra-a1/</link><pubDate>Fri, 31 Dec 2021 14:34:28 +0000</pubDate><guid>/posts/ampere-altra-a1/</guid><description>In early 2020, recent semiconductor startup Ampere announced the Altra, an ultra-dense 80-core ARM64 CPU targeted at cloud computing environments. Patrick Kennedy of ServeTheHome covered the release with an excellent in-depth article last year which I highly recommend reading.
In mid-2020, Oracle became the first cloud provider to add the Ampere Altra to their cloud computing lineup. And in early 2021, Oracle took the unusual step of adding the Altra A1 VMs to their &amp;ldquo;Always Free&amp;rdquo; tier, allowing anyone to create ARM64 VMs with up to 4 cores and 24GB of RAM at no cost.</description><content>&lt;p>In early 2020, recent semiconductor startup Ampere announced the Altra, an ultra-dense 80-core ARM64 CPU targeted at cloud computing environments. Patrick Kennedy of ServeTheHome covered the release with an &lt;a href="https://www.servethehome.com/ampere-altra-80-arm-cores-for-cloud/">excellent in-depth article&lt;/a> last year which I highly recommend reading.&lt;/p>
&lt;p>In mid-2020, Oracle became the first cloud provider to add the Ampere Altra to their cloud computing lineup. And in early 2021, Oracle took the unusual step of adding the Altra A1 VMs to their &amp;ldquo;Always Free&amp;rdquo; tier, allowing anyone to create ARM64 VMs with up to 4 cores and 24GB of RAM at no cost.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/12/Ampere-Altra-Processor-Complex.jpg" />
&lt;figcaption class="center" >Overview of Ampere Altra specs, courtesy of Patrick Kennedy at &lt;a href="https://www.servethehome.com">ServeTheHome&lt;/a>.&lt;/figcaption>
&lt;/figure>
&lt;p>I&amp;rsquo;ve recently started playing around more with my Oracle Cloud account and decided to use Terraform to spin up a free ARM64 VM to compare its performance with one of my existing Raspberry Pi 4Bs.&lt;/p>
&lt;h2 id="specifications">Specifications&lt;/h2>
&lt;p>Oracle allows &amp;ldquo;Always Free&amp;rdquo; tier users to create flexible instance sizes from a total resource pool of 4 &amp;ldquo;OCPUs&amp;rdquo; and an astonishing &lt;strong>24GB&lt;/strong> of RAM. To make the test as fair as possible, I created an instance with 4 OCPUs and 8GB of RAM to match the 8GB RPi 4 that I tested locally.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>System&lt;/strong>&lt;/th>
&lt;th style="text-align:left">&lt;strong>VM.Standard.A1.Flex&lt;/strong>&lt;/th>
&lt;th style="text-align:left">&lt;strong>Raspberry Pi 4B&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">CPU&lt;/td>
&lt;td style="text-align:left">4x Ampere Altra A1 @ 3.0GHz&lt;/td>
&lt;td style="text-align:left">4x ARM Cortex-A72 @ 1.5GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RAM&lt;/td>
&lt;td style="text-align:left">8GB&lt;/td>
&lt;td style="text-align:left">8GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Storage&lt;/td>
&lt;td style="text-align:left">50GB boot volume&lt;/td>
&lt;td style="text-align:left">32GB microSD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">OS&lt;/td>
&lt;td style="text-align:left">Ubuntu 20.04 ARM64&lt;/td>
&lt;td style="text-align:left">Ubuntu 20.04 ARM64&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="benchmark-1---unixbench">Benchmark #1 - UnixBench&lt;/h2>
&lt;p>UnixBench is, as the name implies, a benchmark for Unix systems using standard Unix operations. It&amp;rsquo;s intended to test multiple aspects of the system as a whole, both hardware and software, rather than focusing on any one particular component.&lt;/p>
&lt;p>The standard testing methodology and aggregation of multiple aspects of system performance in a single score make it a very reliable method of judging system performance across platforms and CPU architectures.&lt;/p>
&lt;h4 id="ampere-altra">Ampere Altra&lt;/h4>
&lt;pre tabindex="0">&lt;code> BYTE UNIX Benchmarks (Version 5.1.3)
System: **********: GNU/Linux
OS: GNU/Linux -- 5.11.0-1022-oracle -- #23~20.04.1-Ubuntu SMP Fri Nov 12 15:45:47 UTC 2021
Machine: aarch64 (aarch64)
Language: en_US.utf8 (charmap=&amp;quot;UTF-8&amp;quot;, collate=&amp;quot;UTF-8&amp;quot;)
18:42:15 up 2 min, 1 user, load average: 0.28, 0.27, 0.11; runlevel 2021-12-30
------------------------------------------------------------------------
Benchmark Run: Thu Dec 30 2021 18:42:15 - 19:10:10
4 CPUs in system; running 1 parallel copy of tests
Dhrystone 2 using register variables 41483611.9 lps (10.0 s, 7 samples)
Double-Precision Whetstone 8466.2 MWIPS (9.9 s, 7 samples)
Execl Throughput 5163.9 lps (30.0 s, 2 samples)
File Copy 1024 bufsize 2000 maxblocks 1077137.2 KBps (30.0 s, 2 samples)
File Copy 256 bufsize 500 maxblocks 300831.1 KBps (30.0 s, 2 samples)
File Copy 4096 bufsize 8000 maxblocks 3080256.7 KBps (30.0 s, 2 samples)
Pipe Throughput 1960270.1 lps (10.0 s, 7 samples)
Pipe-based Context Switching 87688.7 lps (10.0 s, 7 samples)
Process Creation 7644.5 lps (30.0 s, 2 samples)
Shell Scripts (1 concurrent) 10008.2 lpm (60.0 s, 2 samples)
Shell Scripts (8 concurrent) 3001.8 lpm (60.0 s, 2 samples)
System Call Overhead 1649115.6 lps (10.0 s, 7 samples)
System Benchmarks Index Values BASELINE RESULT INDEX
Dhrystone 2 using register variables 116700.0 41483611.9 3554.7
Double-Precision Whetstone 55.0 8466.2 1539.3
Execl Throughput 43.0 5163.9 1200.9
File Copy 1024 bufsize 2000 maxblocks 3960.0 1077137.2 2720.0
File Copy 256 bufsize 500 maxblocks 1655.0 300831.1 1817.7
File Copy 4096 bufsize 8000 maxblocks 5800.0 3080256.7 5310.8
Pipe Throughput 12440.0 1960270.1 1575.8
Pipe-based Context Switching 4000.0 87688.7 219.2
Process Creation 126.0 7644.5 606.7
Shell Scripts (1 concurrent) 42.4 10008.2 2360.4
Shell Scripts (8 concurrent) 6.0 3001.8 5003.1
System Call Overhead 15000.0 1649115.6 1099.4
========
System Benchmarks Index Score 1669.7
------------------------------------------------------------------------
Benchmark Run: Thu Dec 30 2021 19:10:10 - 19:38:05
4 CPUs in system; running 4 parallel copies of tests
Dhrystone 2 using register variables 165260124.9 lps (10.0 s, 7 samples)
Double-Precision Whetstone 33900.5 MWIPS (9.9 s, 7 samples)
Execl Throughput 12794.4 lps (30.0 s, 2 samples)
File Copy 1024 bufsize 2000 maxblocks 776156.4 KBps (30.0 s, 2 samples)
File Copy 256 bufsize 500 maxblocks 214667.2 KBps (30.0 s, 2 samples)
File Copy 4096 bufsize 8000 maxblocks 2345518.7 KBps (30.0 s, 2 samples)
Pipe Throughput 7825529.6 lps (10.0 s, 7 samples)
Pipe-based Context Switching 867609.5 lps (10.0 s, 7 samples)
Process Creation 20049.6 lps (30.0 s, 2 samples)
Shell Scripts (1 concurrent) 24471.2 lpm (60.0 s, 2 samples)
Shell Scripts (8 concurrent) 3410.3 lpm (60.0 s, 2 samples)
System Call Overhead 4281214.8 lps (10.0 s, 7 samples)
System Benchmarks Index Values BASELINE RESULT INDEX
Dhrystone 2 using register variables 116700.0 165260124.9 14161.1
Double-Precision Whetstone 55.0 33900.5 6163.7
Execl Throughput 43.0 12794.4 2975.5
File Copy 1024 bufsize 2000 maxblocks 3960.0 776156.4 1960.0
File Copy 256 bufsize 500 maxblocks 1655.0 214667.2 1297.1
File Copy 4096 bufsize 8000 maxblocks 5800.0 2345518.7 4044.0
Pipe Throughput 12440.0 7825529.6 6290.6
Pipe-based Context Switching 4000.0 867609.5 2169.0
Process Creation 126.0 20049.6 1591.2
Shell Scripts (1 concurrent) 42.4 24471.2 5771.5
Shell Scripts (8 concurrent) 6.0 3410.3 5683.8
System Call Overhead 15000.0 4281214.8 2854.1
========
System Benchmarks Index Score 3641.0
&lt;/code>&lt;/pre>&lt;h4 id="raspberry-pi-4">Raspberry Pi 4&lt;/h4>
&lt;pre tabindex="0">&lt;code> BYTE UNIX Benchmarks (Version 5.1.3)
System: **********: GNU/Linux
OS: GNU/Linux -- 5.4.0-1047-raspi -- #52-Ubuntu SMP PREEMPT Wed Nov 24 08:16:38 UTC 2021
Machine: aarch64 (aarch64)
Language: en_US.utf8 (charmap=&amp;quot;UTF-8&amp;quot;, collate=&amp;quot;UTF-8&amp;quot;)
11:40:13 up 3 days, 1:39, 1 user, load average: 1.57, 0.89, 0.74; runlevel 2021-09-07
------------------------------------------------------------------------
Benchmark Run: Thu Dec 30 2021 15:42:07 - 16:10:28
4 CPUs in system; running 1 parallel copy of tests
Dhrystone 2 using register variables 15090554.5 lps (10.0 s, 7 samples)
Double-Precision Whetstone 2682.9 MWIPS (9.9 s, 7 samples)
Execl Throughput 570.5 lps (30.0 s, 2 samples)
File Copy 1024 bufsize 2000 maxblocks 92917.6 KBps (30.0 s, 2 samples)
File Copy 256 bufsize 500 maxblocks 25770.6 KBps (30.0 s, 2 samples)
File Copy 4096 bufsize 8000 maxblocks 273542.6 KBps (30.0 s, 2 samples)
Pipe Throughput 141626.1 lps (10.0 s, 7 samples)
Pipe-based Context Switching 29427.5 lps (10.0 s, 7 samples)
Process Creation 2160.1 lps (30.0 s, 2 samples)
Shell Scripts (1 concurrent) 1980.0 lpm (60.0 s, 2 samples)
Shell Scripts (8 concurrent) 670.6 lpm (60.0 s, 2 samples)
System Call Overhead 170070.1 lps (10.0 s, 7 samples)
System Benchmarks Index Values BASELINE RESULT INDEX
Dhrystone 2 using register variables 116700.0 15090554.5 1293.1
Double-Precision Whetstone 55.0 2682.9 487.8
Execl Throughput 43.0 570.5 132.7
File Copy 1024 bufsize 2000 maxblocks 3960.0 92917.6 234.6
File Copy 256 bufsize 500 maxblocks 1655.0 25770.6 155.7
File Copy 4096 bufsize 8000 maxblocks 5800.0 273542.6 471.6
Pipe Throughput 12440.0 141626.1 113.8
Pipe-based Context Switching 4000.0 29427.5 73.6
Process Creation 126.0 2160.1 171.4
Shell Scripts (1 concurrent) 42.4 1980.0 467.0
Shell Scripts (8 concurrent) 6.0 670.6 1117.6
System Call Overhead 15000.0 170070.1 113.4
========
System Benchmarks Index Score 265.5
------------------------------------------------------------------------
Benchmark Run: Thu Dec 30 2021 16:10:28 - 16:38:54
4 CPUs in system; running 4 parallel copies of tests
Dhrystone 2 using register variables 60067639.9 lps (10.0 s, 7 samples)
Double-Precision Whetstone 10702.3 MWIPS (9.8 s, 7 samples)
Execl Throughput 1995.7 lps (29.9 s, 2 samples)
File Copy 1024 bufsize 2000 maxblocks 181967.0 KBps (30.0 s, 2 samples)
File Copy 256 bufsize 500 maxblocks 48675.5 KBps (30.0 s, 2 samples)
File Copy 4096 bufsize 8000 maxblocks 455783.0 KBps (30.0 s, 2 samples)
Pipe Throughput 559095.7 lps (10.0 s, 7 samples)
Pipe-based Context Switching 127558.3 lps (10.0 s, 7 samples)
Process Creation 5773.4 lps (30.0 s, 2 samples)
Shell Scripts (1 concurrent) 5510.9 lpm (60.0 s, 2 samples)
Shell Scripts (8 concurrent) 746.7 lpm (60.2 s, 2 samples)
System Call Overhead 662741.3 lps (10.0 s, 7 samples)
System Benchmarks Index Values BASELINE RESULT INDEX
Dhrystone 2 using register variables 116700.0 60067639.9 5147.2
Double-Precision Whetstone 55.0 10702.3 1945.9
Execl Throughput 43.0 1995.7 464.1
File Copy 1024 bufsize 2000 maxblocks 3960.0 181967.0 459.5
File Copy 256 bufsize 500 maxblocks 1655.0 48675.5 294.1
File Copy 4096 bufsize 8000 maxblocks 5800.0 455783.0 785.8
Pipe Throughput 12440.0 559095.7 449.4
Pipe-based Context Switching 4000.0 127558.3 318.9
Process Creation 126.0 5773.4 458.2
Shell Scripts (1 concurrent) 42.4 5510.9 1299.7
Shell Scripts (8 concurrent) 6.0 746.7 1244.5
System Call Overhead 15000.0 662741.3 441.8
========
System Benchmarks Index Score 730.7
&lt;/code>&lt;/pre>&lt;h2 id="benchmark-2---sysbench">Benchmark #2 - Sysbench&lt;/h2>
&lt;p>Sysbench is an open-source benchmarking tool that features several independent tests of system hardware performance. The tests I&amp;rsquo;ll focus on are CPU, memory, and file I/O.&lt;/p>
&lt;p>Each test was run on a clean system with no other competing workloads.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:left">&lt;strong>Metric&lt;/strong>&lt;/th>
&lt;th style="text-align:left">&lt;strong>Ampere Altra A1&lt;/strong>&lt;/th>
&lt;th style="text-align:left">&lt;strong>Raspberry Pi 4B&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">CPU&lt;/td>
&lt;td style="text-align:left">primes/sec&lt;/td>
&lt;td style="text-align:left">3511.65&lt;/td>
&lt;td style="text-align:left">1486.26&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RAM&lt;/td>
&lt;td style="text-align:left">throughput, MiB/s&lt;/td>
&lt;td style="text-align:left">4643.80&lt;/td>
&lt;td style="text-align:left">2016.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">File I/O&lt;/td>
&lt;td style="text-align:left">sequential write, MiB/s&lt;/td>
&lt;td style="text-align:left">55.81&lt;/td>
&lt;td style="text-align:left">13.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">File I/O&lt;/td>
&lt;td style="text-align:left">sequential write, iops&lt;/td>
&lt;td style="text-align:left">3551.19&lt;/td>
&lt;td style="text-align:left">797.32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">File I/O&lt;/td>
&lt;td style="text-align:left">random write, MiB/s&lt;/td>
&lt;td style="text-align:left">33.17&lt;/td>
&lt;td style="text-align:left">4.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">File I/O&lt;/td>
&lt;td style="text-align:left">random write, iops&lt;/td>
&lt;td style="text-align:left">2122.76&lt;/td>
&lt;td style="text-align:left">293.73&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="interpreting-the-results">Interpreting the Results&lt;/h1>
&lt;p>Straight away, it&amp;rsquo;s clear that there is a significant performance difference between the admittedly aging ARM Cortex-A72 and the Altra.&lt;/p>
&lt;p>On CPU performance, adjusting for the clock speed difference between the Altra and the Cortex-A72 yields an 18% improvement in IPC. Considering the 4 year age difference between the two chips, it&amp;rsquo;s not earth-shattering - x86 platforms have shown similar trends over the years - but it&amp;rsquo;s still respectable and shows that Ampere has the engineering talent to keep pace with the likes of Intel and AMD.&lt;/p>
&lt;p>Since UnixBench is intended to be a general system benchmark and not purely a measure of CPU muscle, I was most interested to see what else contributed to the whopping 5x increase in the UnixBench index score. For that, I believe the Sysbench memory test points us in the right direction.&lt;/p>
&lt;p>The Sysbench RAM result shows an incredible &lt;strong>130% increase&lt;/strong> in memory throughput on the A1. This result is surprising on its surface - after all, both the Altra and the Cortex-A72 use DDR4 RAM, right? Not quite.&lt;/p>
&lt;p>As a small, low-power/embedded style device, the Raspberry Pi 4 carries the burden of an insanely low power budget - less than 10W. Cutting consumption on the CPU alone isn&amp;rsquo;t enough to meet those demands, however, so the Pi 4 uses a low-power variant of DDR4 called &lt;strong>LPDDR4&lt;/strong>, intended primarily for mobile platforms.&lt;/p>
&lt;p>As discussed in &lt;a href="https://www.hardwaretimes.com/lpddr4-vs-ddr4-vs-lpddr4x-memory-whats-the-difference/">this DDR4 explainer&lt;/a> from HardwareTimes, LPDDR4 makes several key trade-offs for power consumption that result in greatly decreased memory bandwidth. While the Pi 4&amp;rsquo;s DRAM operates at the same 3200MT/s rate as the Altra, its performance is ultimately kneecapped by a single-channel memory implementation and LPDDR4&amp;rsquo;s greatly reduced 32-bit bus width.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/12/50120709591_4c50126bda_c.jpg" />
&lt;figcaption class="center" >The lone LPDDR4 package on the Raspberry Pi 4B.&lt;br/>Photo by &lt;a href="https://flic.kr/p/2jmZJMn">Jeff Geerling&lt;/a>, licensed under &lt;a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0&lt;/a>&lt;/figcaption>
&lt;/figure>
&lt;p>Other factors like File I/O do show significant improvements from the Pi 4, but that seems to be due solely to the Pi 4&amp;rsquo;s microSD card rather than some exceptional storage technology within the Oracle Cloud. Keep in mind that 55MiB/s and 2-3k IOPS is lacking compared to even the lowest consumer-grade SATA 6Gb/s SSDs. It is noteworthy nonetheless that Oracle is able to provide this level of performance in a free cloud server. I fully expect that the Pi 4 would win in this category with a USB3 flash drive or SSD.&lt;/p>
&lt;p>All that being said, I went into this experiment fully expecting to see performance of the free Ampere VM roughly on-par with the Pi 4, at best. My suspicions were based on the anemic performance of other cloud providers' free tier compute instances - AWS&amp;rsquo;s t2.micro, Azure&amp;rsquo;s B1s burstable, and even Oracle&amp;rsquo;s existing AMD Epyc &amp;ldquo;micro&amp;rdquo; shape. The Ampere VMs in Oracle&amp;rsquo;s free tier are surprisingly capable and unlike any other free offering I&amp;rsquo;ve seen to date.&lt;/p></content></item><item><title>Implementing a Private CA for the Home Lab</title><link>/posts/implementing-a-private-ca-for-home-use/</link><pubDate>Sun, 31 Oct 2021 01:29:21 +0000</pubDate><guid>/posts/implementing-a-private-ca-for-home-use/</guid><description>The low risk and low-to-no budget of a home lab environment often results in security taking a back seat. Services are sometimes left open and unguarded in the name of &amp;ldquo;Just Make It Work&amp;rdquo;. Home labs aside, the complexity of running even a halfway-decent security infrastructure makes doing so a non-starter even in many small business environments.
As a result, the largest and most easily exploitable gap you&amp;rsquo;re bound to find in many home labs and small networks is unencrypted traffic.</description><content>&lt;p>The low risk and low-to-no budget of a home lab environment often results in security taking a back seat. Services are sometimes left open and unguarded in the name of &amp;ldquo;Just Make It Work&amp;rdquo;. Home labs aside, the complexity of running even a halfway-decent security infrastructure makes doing so a non-starter even in many small business environments.&lt;/p>
&lt;p>As a result, the largest and most easily exploitable gap you&amp;rsquo;re bound to find in many home labs and small networks is unencrypted traffic. This of course allows for a variety of attack methods against locally-hosted services.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/unencrypted.png" />
&lt;figcaption class="center" >A common private network pitfall - unencrypted traffic and implicit trust&lt;/figcaption>
&lt;/figure>
&lt;p>Let&amp;rsquo;s focus on the two most common attacks - sniffing, where a malicious entity with access to your network devices mirrors packets to their own system to inspect the contents; and Man-in-the-Middle (MitM), where the attacker actually intercepts a traffic flow and impersonates the service on the other end.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/attacks.png" />
&lt;figcaption class="center" >Potential attack vectors - packet sniffing and Man-in-the-Middle&lt;/figcaption>
&lt;/figure>
&lt;p>These are two of the most common attacks around because they&amp;rsquo;re easy to implement and difficult to detect. Fortunately, they are also the easiest to mitigate. First, enforce encryption on the traffic to prevent a sniffing attack; and second, have your services provide a signed certificate to prove that they are who they say they are.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/encrypted.png" />
&lt;figcaption class="center" >The solution - protect traffic with HTTPS backed by a trusted certificate hierarchy&lt;/figcaption>
&lt;/figure>
&lt;p>There are almost as many examples available on the web for enabling HTTPS on a web server as there are web servers themselves. Rather than rehash that process, this post will focus on setting up a &lt;strong>Public Key Infrastructure&lt;/strong> (PKI) to facilitate issuing certificates to services on a local network and provide verification and revocation processes to maintain the integrity of your certificate hierarchy.&lt;/p>
&lt;h2 id="step-1---create-a-root-certificate">Step 1 - Create a Root Certificate&lt;/h2>
&lt;p>All PKIs start with a single self-signed certificate. Because this certificate sits at the top of the hierarchy, it is called a &lt;strong>Root Certification Authority&lt;/strong> or Root CA. Regardless of the size or scope of an organization, a Root CA is nothing more than a single X.509 certificate and private key. The private key is used to sign other certificates, and the certificate carries a fingerprint that uniquely identifies it as being linked to the private key.&lt;/p>
&lt;p>While I could go into great detail showing you how to use the OpenSSL command line to generate all the certificates, that would be a waste of my precious bandwidth. The Google search results for &amp;ldquo;OpenSSL Root CA&amp;rdquo; are littered with hundreds of guides written by people much smarter and more eloquent than me. And since working on a command line is a major part of my profession, I&amp;rsquo;ve come to appreciate a good GUI when one is available.&lt;/p>
&lt;p>For creating and managing a Root CA, I encourage you to check out a fantastic multi-platform application called &lt;a href="https://hohnstaedt.de/xca/">XCA&lt;/a>.&lt;/p>
&lt;p>XCA handles almost every function of the X.509 certificate lifecycle, including creating private keys, generating certificate requests, and signing requests as a self-signed certificate or using an existing certificate key pair stored in XCA&amp;rsquo;s database. I use XCA to manage my Root CA certificate and key, both of which are safely stored in an encrypted offline SQLite database.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/xca_private_key.png" />
&lt;figcaption class="center" >Creating a new EC private key in XCA&lt;/figcaption>
&lt;/figure>
&lt;p>In XCA, I created a private key for my Root CA using the P-521 Elliptic Curve. 521 bits may be absolute overkill for a small home lab environment, but the upside of having a hierarchy is that you will rarely ever be signing things with the Root CA, to the point where the performance impact of choosing P-521 over P-256 is insignificant.&lt;/p>
&lt;p>To create the Root CA, we must also create a Certificate Signing Request. The CSR is signed by the private key generated above, and the resulting certificate will carry the private key&amp;rsquo;s unique fingerprint.&lt;/p>
&lt;p>One of the most helpful features of XCA that I&amp;rsquo;ve come to appreciate is the ability to use templates to generate a certificate signing request. XCA ships with a generic CA template that you can select to generate a functional CA with minimal effort.&lt;/p>
&lt;p>On the Subject tab of the Certificate creation wizard, you&amp;rsquo;re presented with a number of fields that identify who or what the certificate represents. All of these fields are technically optional, but at the very least, you should set a descriptive Common Name to identify your Root CA certificate in browsers and other applications.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/root_ca_1.png" />
&lt;figcaption class="center" >Fill out as much or as little as you'd like - it's your CA!&lt;/figcaption>
&lt;/figure>
&lt;p>On the Extensions tab, you&amp;rsquo;ll set important details like Path Length and the certificate&amp;rsquo;s lifespan. Since this is a Root CA, you should select a reasonably long lifetime. Your intermediate certificates will have shorter lifespans because they&amp;rsquo;ll be used more frequently, and as long as the Root CA is valid, you can always generate another intermediate.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/root_ca_2.png" />
&lt;figcaption class="center" >100 years? Why not - you're in this for the long haul.&lt;/figcaption>
&lt;/figure>
&lt;p>The other fields on this page are not relevant for a Root CA, but will be necessary when creating Intermediates or End Entity certificates if you choose to publish a Certificate Revocation List or OCSP endpoint.&lt;/p>
&lt;p>Once you click OK and sign your certificate, you have a functioning Root CA. You can add this certificate to the trusted certificate store on your systems, and any certificate signed by the Root CA will be inherently trusted as well.&lt;/p>
&lt;p>&lt;strong>Last but not least,&lt;/strong> do not forget to encrypt your XCA database or export the Root CA certificate and key with a strong password. If you lose the key, you lose the ability to sign anything with the Root CA.&lt;/p>
&lt;h2 id="step-2---create-an-intermediate-ca">Step 2 - Create an Intermediate CA&lt;/h2>
&lt;p>In a typical PKI heirarchy, the Root CA never directly signs an &amp;ldquo;End Entity&amp;rdquo; certificate such as a webserver or mTLS client certificate. An &lt;strong>Intermediate CA&lt;/strong> is a certificate that receives its authority from the Root CA, and signs certificates for other subordinate CAs or End Entities directly.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/chain_of_trust.png" />
&lt;/figure>
&lt;p>The process to create an Intermediate CA is almost identical to a Root CA, with the following exceptions:&lt;/p>
&lt;ol>
&lt;li>The Intermediate CA must be signed by the Root CA&amp;rsquo;s private key, giving it a unique cryptographic signature that can be independently verified against the Root CA&amp;rsquo;s public certificate.&lt;/li>
&lt;li>The Intermediate CA usually has a much shorter lifespan than the Root CA, typically on the order of 5-10 years. An Intermediate CA will sign a large number of certificates, and should therefore be recycled more frequently.&lt;/li>
&lt;li>The Intermediate CA certificate should list a &lt;strong>CRL Distribution Point&lt;/strong> - a URL where the Root CA&amp;rsquo;s Certificate Revocation List is published - so clients can confirm that the Root CA has not revoked its signing of the Intermediate CA.&lt;/li>
&lt;li>The Intermediate CA should also manage its own CRL and provide its CRL Distribution Point on all the certificates that it signs.&lt;/li>
&lt;/ol>
&lt;p>Because the Intermediate CA will be used much more frequently, its functions are usually handled by purpose-built server software. There are a number of platforms available - and to be honest, this is where I had the most difficulty. Solutions such as Active Directory Certificate Services, PrimeKey EJBCA, and OpenXPKI are popular for large-scale PKI deployments, but that enterprise-ready approach means they&amp;rsquo;re not well-suited for a minimal deployment, and &amp;ldquo;ease of use&amp;rdquo; is sorely lacking.&lt;/p>
&lt;p>For seamless integration into my existing AD environment, Active Directory Certificate Services was the most logical choice. Regardless of the platform you choose for your intermediate, the process for signing it from your Root CA is mostly unchanged. You&amp;rsquo;ll still be able to generate a request, import it into your Root CA&amp;rsquo;s XCA database to be signed by the Root CA private key, and then export out the generated certificate.&lt;/p>
&lt;h2 id="step-3---create-an-issuing-ca">Step 3 - Create an Issuing CA&lt;/h2>
&lt;p>With a working Root and Intermediate CA in my chain of trust, I wanted to add one final layer of authority to delegate daily signing responsibilities to. For that, I decided to give &lt;a href="https://smallstep.com/certificates/">Smallstep&amp;rsquo;s open-source CA software&lt;/a> a try.&lt;/p>
&lt;p>I originally evaluated Smallstep early on when searching for a product to serve as my Intermediate CA. I didn&amp;rsquo;t consider Step CA to be a serious contender because of its lack of management features and strong emphasis on automation. What eventually won me over when I evaluated it as a potential Issuing CA was its ease of setup and ACME support out of the box.&lt;/p>
&lt;p>&lt;strong>Pros&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Simple command-line setup&lt;/strong> - Smallstep has a number of very useful tutorials, including how to initialize a Step CA as a non-Root authority. For that, you need to create the request using Step and sign it using one of your other certificates.&lt;/li>
&lt;li>&lt;strong>Native ACME support&lt;/strong> - The &lt;code>step-ca&lt;/code> software can be set up to run as a system daemon, and provides a built-in webserver and API to handle certificate requests from clients. Using the provider functionality, it can also be configured as an ACME server similar to LetsEncrypt. This allows me to have a custom, internal-only ACME CA for my local network!&lt;/li>
&lt;li>&lt;strong>Step client daemon mode&lt;/strong> - The &lt;code>step-ca&lt;/code> software allows any valid certificate to renew itself with the CA - no manual renewal or admin intervention required. The &lt;code>step&lt;/code> CLI features a &amp;ldquo;Daemon mode&amp;rdquo; that can run in the background and automatically renew a given certificate when it gets close to expiration.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Cons&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>No GUI&lt;/strong> - If you like having a web interface to view your certificates, Smallstep does allow linking a single &lt;code>step-ca&lt;/code> their Smallstep cloud service. I didn&amp;rsquo;t have the greatest experience managing my CA once I did this, because it meant that some settings in the &lt;code>step-ca&lt;/code> config file were no longer honored and everything had to be managed through their cloud service with OIDC authentication. I rolled the change back and am instead running &lt;code>step-ca&lt;/code> strictly locally.&lt;/li>
&lt;li>&lt;strong>Highly Opinionated Defaults&lt;/strong> - The &lt;code>step&lt;/code> CLI&amp;rsquo;s certificate request function seems to abstract away a lot of otherwise useful X.509 extensions and features. You can still sign a traditional CSR file using the &lt;code>step-ca&lt;/code>, so I still prefer to generate the key and CSR myself to ensure the resulting certificate works for my needs.&lt;/li>
&lt;/ul>
&lt;p>I was very skeptical of Smallstep&amp;rsquo;s product at first, but once I found the right use case for their solution, I was very impressed with the result. I followed their official tutorials on &lt;a href="https://smallstep.com/docs/tutorials/intermediate-ca-new-ca">setting up a new Intermediate CA from an existing Root CA&lt;/a> and &lt;a href="https://smallstep.com/docs/step-ca/certificate-authority-server-production/#running-step-ca-as-a-daemon">running step-ca as a daemon&lt;/a> and was quickly deploying certificates to several servers on my network.&lt;/p>
&lt;p>I even created a &lt;a href="https://gist.github.com/danclough/ee0b25c285cc2fc280e5d2d3a6f855d7">template for a systemd unit file&lt;/a> to run the &lt;code>step&lt;/code> CLI in daemon mode, watching and renewing certificates automatically when they come close to expiring.&lt;/p>
&lt;h2 id="the-final-product">The Final Product&lt;/h2>
&lt;p>The end result of my home lab PKI experiment looks like this:&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/10/hierarchy.png" />
&lt;/figure>
&lt;p>All trust originates with the Root CA, an offline ECC certificate stored in an encrypted database.&lt;/p>
&lt;p>The Root CA signs the Intermediate CA managed by Active Directory Certificate Services, which hosts CRLs and an OCSP service for both the Intermediate and Root CA.&lt;/p>
&lt;p>The Intermediate CA signs the Smallstep CA, which provides CLI-based certificate enrollment and ACME services for local servers and services.&lt;/p>
&lt;p>The Smallstep CA signs dozens of certificates for the various services I run locally, whether issued through the &lt;code>step&lt;/code> CLI or through the native ACME API endpoint. With Smallstep&amp;rsquo;s emphasis on automated issuance and renewals, I&amp;rsquo;ve been able to achieve unbelievably short certificate lifespans of just 24 hours.&lt;/p>
&lt;p>It should be noted that I went through a number of iterations before landing on my current architecture. I spent multiple days researching the array of different PKI products out there, and at least a week wrestling with the setup and configuration of other tools like EJBCA that I ended up scrapping for simplicity&amp;rsquo;s sake. It&amp;rsquo;s perfectly acceptable in a small environment to create a two-tiered hierarchy - for instance, cutting out AD and running a CA hierarchy solely comprised of XCA and Smallstep.&lt;/p>
&lt;p>In the end, what matters most is that you&amp;rsquo;ve got at least some foundation to build upon, no matter how simple or complex it may be.&lt;/p></content></item><item><title>The Case for Home Lab Security</title><link>/posts/case-for-homelab-security/</link><pubDate>Thu, 21 Oct 2021 00:18:34 +0000</pubDate><guid>/posts/case-for-homelab-security/</guid><description>Perhaps unsurprisingly, as my home lab and local area network have matured over the years, both I and my family have come to depend on the assortment of services that I run within the four walls of my home. We begin to take things for granted.
For instance, we put all of our important files - recipes, documents, photos, home videos, etc. - on &amp;ldquo;the server&amp;rdquo;. The contents of said server are invaluable to us, economically and emotionally.</description><content>&lt;p>Perhaps unsurprisingly, as my home lab and local area network have matured over the years, both I and my family have come to depend on the assortment of services that I run within the four walls of my home. We begin to take things for granted.&lt;/p>
&lt;p>For instance, we put all of our important files - recipes, documents, photos, home videos, etc. - on &amp;ldquo;the server&amp;rdquo;. The contents of said server are invaluable to us, economically and emotionally. I take very specific steps to ensure the safety of our data, including ZFS volume-level encryption and frequent encrypted backups to an off-site backup provider. (Shoutout to &lt;a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2&lt;/a>!)&lt;/p>
&lt;p>Having reliable, 24x7 access to all of my family&amp;rsquo;s data is one hell of an accomplishment, but it&amp;rsquo;s not enough any more. I&amp;rsquo;ve seen too many software vulnerabilities in my still-young IT career to be able to blindly trust any single layer of application security. For the same reasons that my data needs to remain safe and accessible, I also need to keep it secure from prying eyes and malicious actors.&lt;/p>
&lt;p>The IT Infosec landscape of today is almost unrecognizable compared to when I started in IT just over 10 years ago. Back then, vendors were clawing their way into our voicemails, inboxes, and conference rooms to pitch some all-encompassing enterprise security platform that promised to keep mysterious hooded hackers out of your unauthenticated SMB 1 file shares and publicly-accessible SharePoint sites.&lt;/p>
&lt;p>Today, the vendors are still there and eager as ever to give you a four-hour sales demo - but as an industry we&amp;rsquo;ve also adopted a lot of common-sense policies that thankfully don&amp;rsquo;t take a multi-million-dollar contract and six figures worth of computing resources to maintain. Namely:&lt;/p>
&lt;ul>
&lt;li>Password-protect everything&lt;/li>
&lt;li>Enable Two-Factor Authentication (2FA)&lt;/li>
&lt;li>Configure Single Sign-On (SSO) wherever possible&lt;/li>
&lt;li>Perhaps most importantly, HTTPS &lt;strong>absolutely everywhere&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>If you follow even just half of the policies above, I promise you - your lab will be just as secure, if not more secure, than the majority of corporate IT environments you&amp;rsquo;ll come across.&lt;/p>
&lt;p>That being said, here are the steps I&amp;rsquo;m taking to secure my home lab:&lt;/p>
&lt;ol>
&lt;li>Implement an internal Public Key Infrastructure (PKI) for my local domain,&lt;/li>
&lt;li>Secure all TLS-capable services on my local network with certificates from the internal PKI, and&lt;/li>
&lt;li>Setup auto-renewal to allow short certificate lifespans (days to weeks) without manual intervention or rotation.&lt;/li>
&lt;/ol>
&lt;p>I fully intend for this blog to be a place for learning and sharing my experiences, so in a coming post I&amp;rsquo;ll attempt to document every step of the process and write about a few of the hurdles I had to overcome. I&amp;rsquo;ll also share my thoughts on a few **** of the fantastic open-source products that I tried out along the way.&lt;/p></content></item><item><title>Kubernetes @ Home</title><link>/posts/kubernetes-at-home/</link><pubDate>Tue, 19 Jan 2021 14:33:00 +0000</pubDate><guid>/posts/kubernetes-at-home/</guid><description>Now that I&amp;rsquo;ve shared some of my physical infrastructure in my home lab, I want to share the service topology for the specific services I run at home for home automation, secure storage, and even this website.
Services As you can see, a number of the specific services I run are focused around infrastructure and system management. Services such as Grafana and Kibana are essential for monitoring and observability for my entire home lab environment.</description><content>&lt;p>Now that I&amp;rsquo;ve shared some of my physical infrastructure in my home lab, I want to share the service topology for the specific services I run at home for home automation, secure storage, and even this website.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/05/service-topology-1.png" />
&lt;/figure>
&lt;h2 id="services">Services&lt;/h2>
&lt;p>As you can see, a number of the specific services I run are focused around infrastructure and system management. Services such as Grafana and Kibana are essential for monitoring and observability for my entire home lab environment. A few of the other services not pictured, for brevity:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>phpIPAM&lt;/strong> - Network IP management and tracking database&lt;/li>
&lt;li>&lt;strong>LDAP Self-Service Portal&lt;/strong> - A simple self-service web application for managing LDAP/Active Directory passwords&lt;/li>
&lt;li>&lt;strong>phpMyAdmin&lt;/strong> - Web-based management interface for my external MariaDB/Galera database cluster&lt;/li>
&lt;/ul>
&lt;h2 id="storage">Storage&lt;/h2>
&lt;p>One recent addition to my lab environment is &lt;a href="https://ceph.io/">Ceph&lt;/a>.&lt;/p>
&lt;p>One of the most difficult and frustrating problems of managing containerized applications across multiple hosts is data persistence and replication - especially when operating on bare metal. In my professional experience managing platforms such as &lt;em>Azure Kubernetes Service&lt;/em>, I&amp;rsquo;ve been able to leverage the built-in StorageClass drivers such as &lt;code>AzureFile&lt;/code> and &lt;em>Azure Managed Disks&lt;/em> for each managed Kubernetes environment.&lt;/p>
&lt;p>When you&amp;rsquo;re on bare metal, however, you&amp;rsquo;re left to fend for yourself. I found Ceph much more complicated to configure at first than GlusterFS, but more resilient and overall easier to integrate into my K8s environment.&lt;/p>
&lt;h2 id="ingress-and-load-balancing">Ingress and Load Balancing&lt;/h2>
&lt;p>I&amp;rsquo;ve been using Traefik as an Ingress Controller for Docker for several years, and it&amp;rsquo;s greatly simplified the ingress routing configuration of each of my services.&lt;/p>
&lt;p>Traefik has built-in ACME support which allows me to utilize Let&amp;rsquo;s Encrypt for solutions that I host on an external domain (i.e., this website and my NextCloud instance). For services on my internal LAN domain, I employ &lt;a href="https://cert-manager.io/">cert-manager&lt;/a> to automatically generate and manage TLS certificates from an internal Certificate Authority based on certificate requests from any workloads deployed to my cluster.&lt;/p>
&lt;p>Similarly to storage, another hard problem of bare-metal K8s is the lack of a network-level load balancer to bring ingress traffic to your cluster. K8s doesn&amp;rsquo;t have any integrated functionality to provide a cluster-wide, externally-accessible IP address for Services. The most effective solution to this problem that I&amp;rsquo;ve found thus far is &lt;a href="https://metallb.universe.tf/">MetalLB&lt;/a>. MetalLB interacts with your external network infrastructure - whether through Layer 2 ARP or Layer 3 routing protocols - to provide Services with an external IP address through the existing LoadBalancer Service type.&lt;/p></content></item><item><title>Migrating Kubernetes from Docker to containerd</title><link>/posts/from-docker-to-containerd/</link><pubDate>Fri, 04 Dec 2020 18:45:04 +0000</pubDate><guid>/posts/from-docker-to-containerd/</guid><description>On December 2nd, a surprise announcement made waves in the Kubernetes Twitter-sphere - that after the upcoming 1.20 release, Docker would be officially deprecated.
Oh no! Due to widespread confusion over what &amp;ldquo;Docker&amp;rdquo; means in specific contexts, many people panicked - myself included. Because of its sheer popularity, Docker has become synonymous with &amp;ldquo;containers&amp;rdquo;. However, Docker is really an entire ecosystem of container tools and processes, including building and shipping container images.</description><content>&lt;p>On December 2nd, a &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">surprise announcement&lt;/a> made waves in the Kubernetes Twitter-sphere - that after the upcoming 1.20 release, Docker would be officially deprecated.&lt;/p>
&lt;h3 id="oh-no">Oh no!&lt;/h3>
&lt;p>Due to widespread confusion over what &amp;ldquo;Docker&amp;rdquo; means in specific contexts, many people panicked - myself included. Because of its sheer popularity, Docker has become synonymous with &amp;ldquo;containers&amp;rdquo;. However, Docker is really an entire ecosystem of container tools and processes, including building and shipping container images. The only thing Kubernetes is deprecating is using Docker as a container runtime, and the reasoning is sound.&lt;/p>
&lt;p>Docker&amp;rsquo;s lack of support for the &amp;ldquo;Container Runtime Interface&amp;rdquo; API - or CRI, for short - forced Kubernetes to implement an abstraction layer called &amp;ldquo;dockershim&amp;rdquo; to allow Kubernetes to manage containers in Docker. The burden of maintaining dockershim was too great to bear, so they are deprecating dockershim in release 1.20, and will eventually remove it entirely in 1.22.&lt;/p>
&lt;p>There are two other container runtimes featured in the Kubernetes quickstart guide as an alternative to Docker - &lt;code>containerd&lt;/code> and CRI-O. &lt;code>containerd&lt;/code> is the same runtime that Docker itself uses internally, just without the fancy Docker wrapping paper and tools.&lt;/p>
&lt;h3 id="ugh">Ugh.&lt;/h3>
&lt;p>Annoyingly enough, I had recently finished migrating my entire homelab container infrastructure to Kubernetes three months ago, with Docker as the container runtime. I initially thought, &amp;ldquo;Crap. Guess I&amp;rsquo;ll be rebuilding my cluster!&amp;rdquo; Then I began to think about what such a change would look like, and whether replacing Docker with &lt;code>containerd&lt;/code> in the same cluster is doable.&lt;/p>
&lt;h3 id="hmm">Hmm&amp;hellip;&lt;/h3>
&lt;p>Turns out, it is!&lt;/p>
&lt;p>I have a 3-node HA cluster which I created using kubeadm. Because I have multiple control plane nodes, I can remove them one at a time using &lt;code>kubeadm reset&lt;/code>, rebuild them with &lt;code>containerd&lt;/code> instead of Docker, and then rejoin using &lt;code>kubeadm join&lt;/code>.&lt;/p>
&lt;p>Here are the steps I came up with:&lt;/p>
&lt;h3 id="uninstalling-docker">Uninstalling Docker&lt;/h3>
&lt;ol>
&lt;li>Using &lt;code>kubectl&lt;/code>, drain and evict pods from the target node.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl drain &lt;span style="color:#e6db74">${&lt;/span>node&lt;span style="color:#e6db74">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>On the target node, use &lt;code>kubeadm&lt;/code> to remove the node from the cluster.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubeadm reset
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>Once &lt;code>kubeadm reset&lt;/code> is finished, stop Docker and finish cleaning up the node.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">systemctl stop docker
rm -rf /etc/cni/net.d
iptables --flush
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>Uninstall the Docker CE suite and CLI.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">apt-get -y remove docker-ce*
rm -rf /var/lib/docker/*
rm -rf /var/lib/dockershim
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="5">
&lt;li>Now&amp;rsquo;s a great time to update your kernel and OS packages&amp;hellip;&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">apt-get update
apt-get -y dist-upgrade
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="6">
&lt;li>&amp;hellip;and reboot!&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">shutdown -r now
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="installing-containerd">Installing containerd&lt;/h3>
&lt;p>(These steps are lifted straight from the fantastic &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd">k8s containerd docs&lt;/a>!)&lt;/p>
&lt;ol start="7">
&lt;li>Apply the module configs for &lt;code>containerd&lt;/code>&amp;rsquo;s required kernel modules.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
&lt;/span>&lt;span style="color:#e6db74">overlay
&lt;/span>&lt;span style="color:#e6db74">br_netfilter
&lt;/span>&lt;span style="color:#e6db74">EOF&lt;/span>
sudo modprobe overlay
sudo modprobe br_netfilter
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="8">
&lt;li>Set sysctl tuning parameters for Kubernetes CRI&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
&lt;/span>&lt;span style="color:#e6db74">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span style="color:#e6db74">net.ipv4.ip_forward = 1
&lt;/span>&lt;span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span style="color:#e6db74">EOF&lt;/span>
sysctl --system
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="9">
&lt;li>Install &lt;code>containerd&lt;/code> if not already installed&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">apt-get install -y containerd.io
&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h5 id="a-note-on-filesystems">A note on filesystems&lt;/h5>
&lt;p>Since these nodes were running Docker, all of the container data is stored in /var/lib/docker. With &lt;code>containerd&lt;/code>, container data is now stored in /var/lib/containerd. If you had the Docker data directory on its own filesystem, you&amp;rsquo;ll need to remove it and create one for &lt;code>containerd&lt;/code>. The exact steps depend in your system, so I won&amp;rsquo;t include them here.&lt;/p>
&lt;h5 id="now-back-to-the-fun">Now back to the fun!&lt;/h5>
&lt;hr>
&lt;ol start="10">
&lt;li>Generate a default configuration:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">mkdir -p /etc/containerd
containerd config default &amp;gt; /etc/containerd/config.toml
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="11">
&lt;li>Modify the &lt;code>config.toml&lt;/code> file generated above to enable the systemd cgroup driver:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-toml" data-lang="toml">[&lt;span style="color:#a6e22e">plugins&lt;/span>.&lt;span style="color:#e6db74">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.&lt;span style="color:#a6e22e">containerd&lt;/span>.&lt;span style="color:#a6e22e">runtimes&lt;/span>.&lt;span style="color:#a6e22e">runc&lt;/span>]
...
[&lt;span style="color:#a6e22e">plugins&lt;/span>.&lt;span style="color:#e6db74">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.&lt;span style="color:#a6e22e">containerd&lt;/span>.&lt;span style="color:#a6e22e">runtimes&lt;/span>.&lt;span style="color:#a6e22e">runc&lt;/span>.&lt;span style="color:#a6e22e">options&lt;/span>]
&lt;span style="color:#a6e22e">SystemdCgroup&lt;/span> = &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="12">
&lt;li>Now enable and start &lt;code>containerd&lt;/code>!&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">systemctl enable --now containerd
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="13">
&lt;li>Make sure &lt;code>containerd&lt;/code> is happy before proceeding.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">systemctl status containerd
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your system is now fully configured with the &lt;code>containerd&lt;/code> runtime - but before we rejoin the cluster, there&amp;rsquo;s one more step to get kubeadm to play nicely with it!&lt;/p>
&lt;h3 id="updating-the-kubelet-configuration">Updating the kubelet configuration&lt;/h3>
&lt;p>Since this cluster was originally built with the Docker runtime, the default kubelet configuration does not explicitly set a cgroup driver. By default, kubeadm with Docker auto-detects the cgroup driver - but other runtimes like &lt;code>containerd&lt;/code> don&amp;rsquo;t support that yet. As a result, when you &lt;code>kubeadm join&lt;/code> a &lt;code>containerd&lt;/code> node without a cgroup driver specified, the kubelet won&amp;rsquo;t start. You can ninja-edit the &lt;code>/var/lib/kubelet/config.yaml&lt;/code> file when joining and then restart the kubelet, but that&amp;rsquo;s tedious and unnecessary.&lt;/p>
&lt;p>Fortunately, we can update the baseline kubelet config at the cluster level to specify the right cgroup driver to use.&lt;/p>
&lt;ol start="14">
&lt;li>Edit the baseline kubelet config for your Kubernetes version - 1.18, 1.19, etc.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl edit cm -n kube-system kubelet-config-1.18
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="15">
&lt;li>Add the following entry for &lt;code>cgroupDriver&lt;/code>:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#f92672">data&lt;/span>:
&lt;span style="color:#f92672">kubelet&lt;/span>: |&lt;span style="color:#e6db74">
&lt;/span>&lt;span style="color:#e6db74"> ...
&lt;/span>&lt;span style="color:#e6db74"> cgroupDriver: systemd
&lt;/span>&lt;span style="color:#e6db74"> ...&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="joining-the-cluster">Joining the cluster&lt;/h3>
&lt;ol start="16">
&lt;li>Proceed to &lt;code>kubeadm join&lt;/code> your node with the appropriate kubeadm command! You can run &lt;code>kubeadm token create --print-join-command&lt;/code> to create a new token.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubeadm join 123.45.67.89:6443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --token &amp;lt;...snip...&amp;gt; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --discovery-token-ca-cert-hash sha256:&amp;lt;...snip...&amp;gt; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">[&lt;/span>--control-plane --certificate-key &amp;lt;...snip...&amp;gt;&lt;span style="color:#f92672">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For control plane nodes, be sure to include the &lt;code>--control-plane&lt;/code> flag and &lt;code>--certificate-key&lt;/code> for your cluster - otherwise the node will join as a worker! I made this mistake and had to re-reset and rejoin the first node I converted. Use &lt;code>kubeadm init phase upload-certs --upload-certs&lt;/code> on another control plane node to reupload your certificates to the cluster, and then pass the provided certificate key to &lt;code>kubeadm join&lt;/code>.&lt;/p>
&lt;h3 id="clean-up">Clean-up&lt;/h3>
&lt;p>Once your new node is joined, wait a few minutes for your CNI plugin to reprovision the networking stack. Once you&amp;rsquo;re satisfied and the node shows &lt;code>Ready&lt;/code> in &lt;code>kubectl get nodes&lt;/code>, you can uncordon the node with &lt;code>kubectl uncordon&lt;/code>.&lt;/p>
&lt;p>And finally, if necessary, don&amp;rsquo;t forget to re-taint your new control plane node! When &lt;code>kubeadm&lt;/code> rejoins the node, it applies the same default restriction to prevent control plane nodes from running worker pods. I find separate control planes unnecessary for my homelab, so I taint them to allow pods to run anywhere.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="final-thoughts">Final thoughts&lt;/h3>
&lt;p>Now, granted - this process is &lt;em>extremely&lt;/em> unnecessary, and runs contrary to the cloud ethos that nodes should be treated like cattle. But for someone running a small bare-metal environment - where provisioning new nodes &lt;em>isn&amp;rsquo;t&lt;/em> entirely automated - these steps save a lot of time otherwise spent rebuilding VMs from the ground up, assigning IP addresses, updating DNS, and potentially building a whole new cluster.&lt;/p>
&lt;p>And as an &lt;em>added&lt;/em> bonus, I now know more about Kubernetes and container runtimes than I did last week.&lt;/p></content></item><item><title>What's In My Lab</title><link>/posts/whats-in-my-lab/</link><pubDate>Sun, 04 Oct 2020 02:14:16 +0000</pubDate><guid>/posts/whats-in-my-lab/</guid><description>Like many people in IT, I&amp;rsquo;ve been running a home lab for several years. My home lab has become progressively more complicated over the years as I&amp;rsquo;ve layered in new technologies that I want to explore and added new services to my home network.
Hardware My home lab runs mainly on low-power Dell OptiPlex ultra-small-form-factor (USFF) hardware. I wanted this lab to be always-on - so small, quiet, and energy-efficient hardware is a must-have.</description><content>&lt;p>Like many people in IT, I&amp;rsquo;ve been running a home lab for several years. My home lab has become progressively more complicated over the years as I&amp;rsquo;ve layered in new technologies that I want to explore and added new services to my home network.&lt;/p>
&lt;h3 id="hardware">Hardware&lt;/h3>
&lt;p>My home lab runs mainly on low-power Dell OptiPlex ultra-small-form-factor (USFF) hardware. I wanted this lab to be always-on - so small, quiet, and energy-efficient hardware is a must-have. The notable exception is my NAS, a 2U Dell PowerEdge rackmount server with an array of 3.5&amp;quot; SAS spinning disks striped in a large ZFS RAID-Z2 pool.&lt;/p>
&lt;h3 id="hypervisors">Hypervisors&lt;/h3>
&lt;p>For several years, I ran VMware ESXi mostly as a learning opportunity. Over time I found that the overhead of running ESXi on low-spec commodity hardware was too great, and decided to shift my workloads over to &lt;a href="https://www.proxmox.com/">Proxmox VE&lt;/a>.&lt;/p>
&lt;figure class="center" >
&lt;img src="/images/2021/05/infra_services.png" />
&lt;/figure>
&lt;h3 id="services">Services&lt;/h3>
&lt;p>Each physical system in my homelab runs the Debian-based Proxmox VE hypervisor. The hypervisors host a mix of LXC-based containers and QEMU virtual machines. From top to bottom:&lt;/p>
&lt;ul>
&lt;li>HAProxy
&lt;ul>
&lt;li>Two very lightweight LXD containers serving as a virtual network load balancer&lt;/li>
&lt;li>HAProxy acts as a TCP and HTTP load balancer&lt;/li>
&lt;li>Keepalived manages virtual IPs that failover between the two containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MariaDB Galera Cluster
&lt;ul>
&lt;li>Three Debian LXD containers running MariaDB with Galera multi-master replication&lt;/li>
&lt;li>Clients access the MariaDB cluster using a VIP and server pool managed by the HAProxy cluster&lt;/li>
&lt;li>I wrote a &lt;a href="https://github.com/danclough/mysql-healthcheck">monitoring daemon in Go&lt;/a> that provides HTTP healthcheck capabilities for HAProxy to determine the health of each MariaDB instance&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ceph
&lt;ul>
&lt;li>Each Proxmox host runs a Ceph monitor&lt;/li>
&lt;li>Each host also hosts a Ceph OSD on an internal 2.5&amp;quot; SATA disk&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Active Directory
&lt;ul>
&lt;li>Three MSDN Server 2019 Core VMs across each host provide Active Directory services&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kubernetes
&lt;ul>
&lt;li>3 control plane nodes with taints&lt;/li>
&lt;li>1 worker node&lt;/li>
&lt;li>Workloads on Kubernetes are able to leverage the external MariaDB and Ceph clusters for persistence&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In addition to these clustered services, I also deployed a few standalone services for better management and visibility across my environment:&lt;/p>
&lt;ul>
&lt;li>Chef Infra Server
&lt;ul>
&lt;li>Manages automation for VMs, LXD containers, and physical hosts&lt;/li>
&lt;li>I&amp;rsquo;ve also open-sourced &lt;a href="https://github.com/danclough/chef-qemu_guest">a few of the cookbooks&lt;/a> I created to manage my lab systems&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Elastic ELK Stack
&lt;ul>
&lt;li>Logstash takes syslog data and Kubernetes logs from Filebeat and indexes them into Elasticsearch&lt;/li>
&lt;li>A Kibana instance running on Kubernetes provides search and visualization capabilities&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>InfluxDB
&lt;ul>
&lt;li>Aggregates time-series monitoring data from telegraf clients on Linux&lt;/li>
&lt;li>Provides data to a Grafana instance running on Kubernetes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>Designing a Homelab</title><link>/posts/designing-a-homelab/</link><pubDate>Sun, 30 Aug 2020 12:43:52 +0000</pubDate><guid>/posts/designing-a-homelab/</guid><description>Just as a motorhead might get their endorphine fix attending a vintage car show, I browse r/homelab almost daily and fawn over the beautifully-curated racks full of enterprise grade hardware sitting in some random person&amp;rsquo;s apartment. I often get the feeling that I won&amp;rsquo;t be truly satisfied until I have a few racks of my own, full of routers, switches, servers, and disk shelves.
What would I do with them? Aside from stress-testing my home&amp;rsquo;s electrical wiring, I really don&amp;rsquo;t know.</description><content>&lt;p>Just as a motorhead might get their endorphine fix attending a vintage car show, I browse &lt;a href="https://reddit.com/r/homelab">r/homelab&lt;/a> almost daily and fawn over the beautifully-curated racks full of enterprise grade hardware sitting in some random person&amp;rsquo;s apartment. I often get the feeling that I won&amp;rsquo;t be truly satisfied until I have a few racks of my own, full of routers, switches, servers, and disk shelves.&lt;/p>
&lt;p>What would I do with them? Aside from stress-testing my home&amp;rsquo;s electrical wiring, I really don&amp;rsquo;t know.&lt;/p>
&lt;p>And with that, we have our first piece of advice for beginning homelabbers&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Don&amp;rsquo;t Overthink It&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>Browsing &lt;a href="https://reddit.com/r/homelab">r/homelab&lt;/a> is a blessing and a curse. You want the shinys and the blinkenlights, but beauty is only skin deep.&lt;/p>
&lt;ol start="2">
&lt;li>&lt;strong>Be Goal-Oriented&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>What do you want or need from your first lab? Consider the examples below and good, inexpensive options for each.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Designing web sites&lt;/strong> - static pages, PHP, Node.JS, etc.
&lt;ul>
&lt;li>Raspberry Pi 3 or 4 - $40&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Small low-power media server&lt;/strong> - Plex, XBMC, etc.
&lt;ul>
&lt;li>Raspberry Pi 4 (4GB) with a USB hard drive - $80&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Working with containers&lt;/strong> - Docker, Docker Compose
&lt;ul>
&lt;li>Raspberry Pi 4 (4-8GB) with a USB hard drive or SSD - $140
&lt;ul>
&lt;li>The 64-bit arm64 architecture is supported by many popular containers on Docker Hub and other public registries.&lt;/li>
&lt;li>RPi 4 supports booting from USB devices rather than unreliable MicroSD, alleviating a common complaint of Pi users with write-heavy workloads.&lt;/li>
&lt;li>The newest Pi 4 SKU comes with 8GB RAM - plenty of headroom for running multiple containers.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Small NUC-sized x86-64 PC - $200&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Running VMs&lt;/strong>
&lt;ul>
&lt;li>Any x86-64 PC running Proxmox VE, a free Linux QEMU-based hypervisor&lt;/li>
&lt;li>Proxmox VE&amp;rsquo;s hardware support is only limited by what Debian supports - which, as it turns out, is &lt;em>a lot&lt;/em>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>&lt;strong>Don&amp;rsquo;t Be Afraid To Break It&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>As a hands-on learner, I retain information most effectively when I can apply it to a real problem. As you change up, tear down, and rebuild the parts of your homelab, you might find a new solution to an old problem, or think up a new way of doing something you did before.&lt;/p>
&lt;p>The beauty of a homelab is all right there in the name - &lt;strong>home&lt;/strong> &lt;strong>lab&lt;/strong>. It&amp;rsquo;s yours to break and fix as you see fit, for learning or just for fun. No service-level agreements to worry about, no stakeholders to notify, no change control processes to adhere to.&lt;/p>
&lt;p>If you come to depend on the services you run in your homelab, start focusing your efforts on high availability. A few examples to note:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Failover&lt;/strong> - Set up a load balancer to have one IP or web URL point to two servers, so when one goes down the other takes over. Often this happens without you even knowing, so set up monitoring while you&amp;rsquo;re at it to know when a server experiences issues.&lt;/li>
&lt;li>&lt;strong>Fault Tolerance -&lt;/strong> Are all your files stored on one hard drive? Try setting up a RAID mirror so your services keep running if a drive dies. Do you frequently experience power outages? Install an &lt;strong>Uninterruptable Power Supply (UPS)&lt;/strong> to provide backup power to your devices during a utility blackout.&lt;/li>
&lt;li>&lt;strong>Disaster Recovery -&lt;/strong> How quickly can you get things back up and running after a server failure? **** Copy your services or container configs to another server using a scheduled backup process, and test the restore process periodically to ensure that it works.&lt;/li>
&lt;/ul></content></item><item><title>The Importance of Homelabs</title><link>/posts/importance-of-homelabs/</link><pubDate>Sun, 23 Aug 2020 18:04:03 +0000</pubDate><guid>/posts/importance-of-homelabs/</guid><description>&amp;ldquo;Was that in the notes?&amp;rdquo;
&amp;ldquo;Where&amp;rsquo;d you learn that?&amp;rdquo;
&amp;ldquo;How do you just know that?&amp;rdquo;
These are all questions I&amp;rsquo;ve been asked, in one form or another, over the last 10 years of working in IT. The situations vary, but there&amp;rsquo;s a common thread to each - an issue identified, a problem solved, a better way to complete some task.
Most of the time, I don&amp;rsquo;t answer. How do you explain that you knew that one specific thing because of the three hours you spent last month sitting in your cold basement, begging your production NAS to start back up after an upgrade?</description><content>&lt;p>&amp;ldquo;Was that in the notes?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;Where&amp;rsquo;d you learn that?&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;How do you just &lt;em>know&lt;/em> that?&amp;rdquo;&lt;/p>
&lt;p>These are all questions I&amp;rsquo;ve been asked, in one form or another, over the last 10 years of working in IT. The situations vary, but there&amp;rsquo;s a common thread to each - an issue identified, a problem solved, a better way to complete some task.&lt;/p>
&lt;p>Most of the time, I don&amp;rsquo;t answer. How do you explain that you knew that &lt;em>one specific thing&lt;/em> because of the three hours you spent last month sitting in your cold basement, begging your production NAS to start back up after an upgrade? Well, it&amp;rsquo;s not &lt;em>production, per se&lt;/em>&amp;hellip; but to your family or roommates, it may as well be. &amp;ldquo;It&amp;rsquo;s a long story. I&amp;rsquo;ve seen it before. Not here - on my server at home.&amp;rdquo;&lt;/p>
&lt;p>Engineering knowledge is a perishable resource. Its decay comes not just from continual technological advancement, but from your own experiential half-life as well. I haven&amp;rsquo;t touched a Cisco device in over a year and my last Cisco cert has since lapsed. I could still poke around in the IOS CLI with a little fumbling and an embarrassing overuse of the &lt;code>?&lt;/code>, but I doubt I could set up an eBGP neighbor on-demand without a reference page and a few minutes of review to jog my memory.&lt;/p>
&lt;p>Other times, it&amp;rsquo;s not even the knowledge itself that you benefit from. The information is out there - you&amp;rsquo;re free to access it as needed. The concepts __ are what you really need to succeed. You&amp;rsquo;ll have a much harder time accomplishing things with the reference pages and a few minutes of study when you never had the opportunity to explore routing and BGP concepts in the first place.&lt;/p>
&lt;p>Having a homelab is a quick and easy way to stand out as a professional. That statement holds true whether you&amp;rsquo;re a specialist or a generalist, a 10-year veteran or just starting out in Tech.&lt;/p></content></item><item><title>About me</title><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>I&amp;rsquo;m a developer, DevOps engineer, and technology architect. I have specific interests in networking, cloud infrastructure and architecture, and distributed systems.
Most of my attention these days has been focused on Kubernetes and cloud-native computing. I&amp;rsquo;m actively looking for open-source projects to contribute to.
If you&amp;rsquo;d like to see more of me, follow me on Twitter or check out my GitHub.</description><content>&lt;p>I&amp;rsquo;m a developer, DevOps engineer, and technology architect. I have specific interests in networking, cloud infrastructure and architecture, and distributed systems.&lt;/p>
&lt;p>Most of my attention these days has been focused on Kubernetes and cloud-native computing. I&amp;rsquo;m actively looking for open-source projects to contribute to.&lt;/p>
&lt;p>If you&amp;rsquo;d like to see more of me, follow me on Twitter or check out my GitHub.&lt;/p></content></item></channel></rss>